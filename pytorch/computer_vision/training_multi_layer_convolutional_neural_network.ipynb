{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the [Convolutional Neural Network](https://www.kaggle.com/code/aisuko/convolutional-neural-network) notebook, we have learned about convolutional filters that can extract patterns from images. For our MNIST classifier we used 9x5x5 filtersk resulting in 9x24x24 tensor.\n",
    "\n",
    "We can use the same idea of convolution to extract higher-level patterns in the image. For example, rounded edges of digits such as 8 and 9 can be composed from a number of smaller strokes. To recognize those patterns, we can build another layer of convolution filters on top of the result of the first layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from torchinfo import summary\n",
    "import numpy as np\n",
    "\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the paltform, Apple Silicon or Linux\n",
    "import os, platform\n",
    "\n",
    "torch_device=\"cpu\"\n",
    "\n",
    "if 'kaggle' in os.environ.get('KAGGLE_URL_BASE','localhost'):\n",
    "    torch_device = 'cuda'\n",
    "else:\n",
    "    torch_device = 'mps' if platform.system() == 'Darwin' else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "data_test = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(data_train, batch_size=128)\n",
    "test_loader = DataLoader(data_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-layer perceptron\n",
    "\n",
    "In a multi-layer network, we will add one or more `hidden layers`,\n",
    "\n",
    "<figure><img src=\"https://hostux.social/system/media_attachments/files/110/733/604/476/365/268/original/3f1163265cf9b88c.png\" alt=\"\" ><figcaption><p>Source from Microsoft learning</p></figcaption></figure>\n",
    "\n",
    "This layer may contain any number of neurons, which will affect how powerful our neural network it, i.e. how many parameters will it have. The more parameters there are in the network, the more data we need to train it.\n",
    "\n",
    "However, `more` is not always better. A number of parameters of a neural network should be chosen depending on the dataset size, to prevent `overfitting`. \n",
    "\n",
    "> Overfitting is a phenomenon when a neural network learns to recognize the training data very well, but fails to generalize to new data. In other words, it memorizes the training data, but does not learn to recognize the patterns in the data.\n",
    "\n",
    "Our network layer structure will look like this:\n",
    "\n",
    "<figure><img src=\"https://hostux.social/system/media_attachments/files/110/733/630/147/169/801/original/21a55f4237dda6ee.png\" alt=\"\" ><figcaption><p>Source from Microsoft learning</p></figcaption></figure>\n",
    "\n",
    "An important thing to note here is the non-linear activation function layer, called `ReLU`. It is important to introduce those non-linear activation functions, because they are one of the reasons neural networks achieve high expressive power. Indeed, it can be demonstrated mathematically that if a network consisted just of a series of linear layers, it would essentially  be equivalent to one linear layer. Thus inserting non-linear functions in beetween layers is important!\n",
    "\n",
    "Here how two of the most frequently used activation function look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_function(f, name=''):\n",
    "    plt.plot(range(-10,10), [f(torch.tensor(x, dtype=torch.float32)).item() for x in range(-10,10)]) #f is used to plot the function\n",
    "    plt.title(name)\n",
    "\n",
    "# The plt.subplot(121) call is equivalent to plt.subplot(1, 2, 1). It specifies a grid of 1 row and 2 columns, and the subplot is placed in the first position (index 1) of the grid\n",
    "plt.subplot(121) # 121 means 1 row, 2 columns, 1st subplot\n",
    "plot_function(torch.relu, 'ReLU')\n",
    "plt.subplot(122)\n",
    "plot_function(torch.sigmoid, 'Sigmoid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our network can be defined in PyTorch in the following way, using `Sequential` syntax:\n",
    "\n",
    "In multi-class classification problems, `nn.Softmax(dim=1)` is typically used to normalize the output probabilities for each sample in the batch. On the other hand, `nn.Softmax(dim=0)` can be used to normalize the output probabilities across all samples in the batch, which can be useful in some cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(28*28, 100), # 784 is the input size, 100 is the output size\n",
    "    nn.ReLU(),             # ReLU is the activation function\n",
    "    nn.Linear(100, 10),    # 100 is the input size, 10 is the output size\n",
    "    nn.Softmax(dim=0)\n",
    ").to(torch_device)\n",
    "\n",
    "summary(net, input_size=(1, 28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use `torchsummary.summary()` function to display a detailed layer-by-layer structure of a network with some other useful information. In particular, we can see the number of parameters of the network.\n",
    "\n",
    "Let's train this multi-layerd perceptron:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(net, dataloader, lr=0.01, optimizer=None, loss_fn=nn.NLLLoss()):\n",
    "    optimizer = optimizer or torch.optim.Adam(net.parameters(), lr=lr) # use Adam optimizer if not provided\n",
    "    net.train() # put the network into training mode make sure the parameters are trainable\n",
    "    total_loss,acc,count =0,0,0\n",
    "    for features, labels in dataloader:\n",
    "        features,labels = features.to(torch_device), labels.to(torch_device)\n",
    "        optimizer.zero_grad() # reset the gradients to zero before each batch to avoid accumulation\n",
    "        out=net(features) # forward pass of the mini-batch through the network to obtain the outputs\n",
    "        loss=loss_fn(out,labels) # compute the loss\n",
    "        loss.backward() # compute the gradients of the loss with respect to all the parameters of the network\n",
    "        optimizer.step() # update the parameters of the network using the gradients to minimize the loss\n",
    "        total_loss+=loss # accumulate the loss for inspection\n",
    "        _,preds=torch.max(out,dim=1) # compute the predictions to obtain the accuracy\n",
    "        acc+=(preds==labels).sum() # accumulate the correct predictions\n",
    "        count+=len(labels) # accumulate the total number of examples\n",
    "    return total_loss.item()/count, acc.item()/count # return the loss and accuracy\n",
    "\n",
    "def validate(net, dataloader, loss_fn=nn.NLLLoss()):\n",
    "    net.eval() # put the network into evaluation mode to deactivate the dropout layers\n",
    "    count,acc,loss =0,0,0\n",
    "    with torch.no_grad(): # deactivate autograd to save memory and speed up computations\n",
    "        for features, labels in dataloader:\n",
    "            features,labels = features.to(torch_device), labels.to(torch_device)\n",
    "            out=net(features) # forward pass of the mini-batch through the network to obtain the outputs\n",
    "            loss += loss_fn(out,labels) # compute the loss\n",
    "            preds=torch.max(out,dim=1)[1] # compute the predictions to obtain the accuracy\n",
    "            acc+=(preds==labels).sum() # accumulate the correct predictions\n",
    "            count+=len(labels) # accumulate the total number of examples\n",
    "    return loss.item()/count, acc.item()/count # return the loss and accuracy\n",
    "\n",
    "def train(net, train_loader, test_loader, optimizer=None, lr=0.01, epochs=10, loss_fn=nn.NLLLoss()):\n",
    "    optimizer = optimizer or torch.optim.Adam(net.parameters(), lr=lr) # use Adam optimizer if not provided\n",
    "    res = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc':[]}\n",
    "    for ep in range(epochs):\n",
    "        tl, ta = train_epoch(net, train_loader, optimizer=optimizer,lr=lr, loss_fn=loss_fn)\n",
    "        vl,va = validate(net, test_loader, loss_fn=loss_fn)\n",
    "        print(f\"Epoch {ep:2}, Train acc={ta:.3f}, Val acc={va:.3f}, Train loss={tl:.3f}, Val loss={vl:.3f}\")\n",
    "        res['train_loss'].append(tl)\n",
    "        res['train_acc'].append(ta)\n",
    "        res['val_loss'].append(vl)\n",
    "        res['val_acc'].append(va)\n",
    "    return res\n",
    "\n",
    "def plot_results(hist):\n",
    "    plt.figure(figsize=(15,5)) # figure is used to create a new figure\n",
    "    plt.subplot(121) # subplot is used to create a new subplot on a grid\n",
    "    plt.plot(hist['train_acc'], label='Training acc')\n",
    "    plt.plot(hist['val_acc'], label='Validation acc')\n",
    "    plt.legend() # legend is used to add a legend to the plot\n",
    "    plt.subplot(122) # plot the loss\n",
    "    plt.plot(hist['train_loss'], label='Training loss')\n",
    "    plt.plot(hist['val_loss'], label='Validation loss')\n",
    "    plt.legend()\n",
    "\n",
    "hist = train(net, train_loader, test_loader, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note the following:\n",
    "\n",
    "* This network is more expressive than the one layered perceptron we have trained in the previous notebook. Thus it achieves a much higher training accuracy and given sufficiently large number of parameters - it can get to almost 100%\n",
    "\n",
    "* Onece the validation accuracy stops increasing - it means that the model has reached it's ability to generalize, and further training will likely to result in overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class-based network definitions\n",
    "\n",
    "Defining models using a `Sequential` style as a list of layers seems very convenient but it is somewhat limited. At some point you may need to define more complex networks, which contain shared weights, or some non-linera connections between layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import relu, log_softmax\n",
    "\n",
    "class MyNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyNet, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.hidden = nn.Linear(28*28, 100)\n",
    "        self.out = nn.Linear(100, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x =self.flatten(x)\n",
    "        x = self.hidden(x)\n",
    "        x = relu(x)\n",
    "        x = self.out(x)\n",
    "        x = log_softmax(x, dim=0)\n",
    "        return x\n",
    "\n",
    "net = MyNet().to(torch_device)\n",
    "\n",
    "summary(net, input_size=(1, 28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the structure of a neural network is the same as the `Sequential` defined network, but the definition is more explicit. Our custom neural network is represented by a class inherited from `torch.nn.Module` class.\n",
    "\n",
    "Class definition consists of two  parts:\n",
    "* In the constructor `__init__` we define all layers that our network will have. Those layers are stored as internal variables of the class, and PyTorch will automatically know that parameters of those layers shoudl be optimized when training. Internally, PyTorch uses `parameters()` method to look for all trainable parameters, and `nn.Module` will automatically collect all trainable parameters from all sub-modules.\n",
    "* We define the `forward` method that does the forward pass computation of out neural network. In our case, we start with a parameter tensor `x`, and explicitly pass it thorugh all the layers and activation functions, starting from `flatten`, up to the final layer `out`. When we apply our neural network to some input data `x` by writing `out = net(x)`, the `forward` methos is called.\n",
    "\n",
    "In fact, `Sequential` networks are represented in a very similar manner, they just store a list of layers and apply them sequentially during the forward pass. Here we have a chance to represent this process more ecplicity, which evetually gives us more flexibility. That is one of the reasons that using classes for neural network definition is a recommended and preferred practice.\n",
    "\n",
    "Now we will train our network and make sure we get similar results as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = train(net, train_loader, test_loader, epochs=5)\n",
    "plot_results(hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-level networks can achieve higher accuracy than single-layer perceptron, however, they are not perfect for computer vision tasks. In images, there are some structural patterns that can help us classify an obejct regardless of it's position in the image, but perceptrons do not allow us to extract those patterns and look for them selectively."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
