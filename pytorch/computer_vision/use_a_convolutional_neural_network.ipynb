{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional nerual network\n",
    "\n",
    "In the [Training a simple dense neural network](https://www.kaggle.com/code/aisuko/training-a-simple-dense-neural-network) using class definition, but those networks were generic, and not specialized for computer vision tasks. This time we will trying to define `Convolutional Neural Networks(CNNs)`, which are specifically designed for computer vision.\n",
    "\n",
    "Computer vision is different from generic classification, because when we are trying to fing a certain object in the picture, we are scanning the image looking for some specific `patterns` and their combinations. For example, when looking for a catm,, we first may look for horizontal lines, which can form whiskers, and then certain combinations of whiskers can tell us that it is actually a picture of a cat. Relative position and presence of certain patterns is important, and not their exact position on the image.\n",
    "\n",
    "To extract patterns, we will use the notion of `convolutional filters`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the paltform, Apple Silicon or Linux\n",
    "import os, platform\n",
    "\n",
    "torch_device=\"cpu\"\n",
    "\n",
    "if 'kaggle' in os.environ.get('KAGGLE_URL_BASE','localhost'):\n",
    "    torch_device = 'cuda'\n",
    "else:\n",
    "    torch_device = 'mps' if platform.system() == 'Darwin' else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from torchinfo import summary\n",
    "import numpy as np\n",
    "\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "data_test = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Peek digit dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Digit to be predicted:',data_train[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(data_train, batch_size=128)\n",
    "test_loader = torch.utils.data.DataLoader(data_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional filters\n",
    "\n",
    "Convolutional filters are small windows that run over each pixel of the image and weighted average of the neighboring pixels. Here are the examples of applying two different convolutional filters over our MNIST handwritten digits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_convolution(t, title=''):\n",
    "    with torch.no_grad():\n",
    "        # nn.Con2d is used to perform the convolution\n",
    "        # * kernel_size is the size of the filter, filter is a 3x3 matrix\n",
    "        # * out_channels is the number of filters to use, this corresponds to the number of output channels\n",
    "        # * in_channels is the number of channels in the input image, this corresponds to the number of input channels\n",
    "        # * the channels are the colors, so for a color image, the number of channels is 3 (red, green, blue)\n",
    "        # * for a grayscale image, the number of channels is 1\n",
    "        c = nn.Conv2d(kernel_size=(3,3), out_channels=1, in_channels=1)\n",
    "        c.weight.copy_(t) # copy the filter to the convolution layer\n",
    "        fig, ax=plt.subplots(2,6,figsize=(8,3)) # create a figure with 2 rows and 6 columns, each plot is 8x3\n",
    "        fig.suptitle(title, fontsize=16) # set the title of the figure\n",
    "        for i in range(5):\n",
    "            im = data_train[i][0] # get the image\n",
    "            ax[0][i].imshow(im[0]) # plot the image\n",
    "            ax[1][i].imshow(c(im.unsqueeze(0))[0][0]) # plot the convolution\n",
    "            ax[0][1].axis('off') # turn off the axis\n",
    "            ax[1][1].axis('off') # turn off the axis\n",
    "        ax[0,5].imshow(t) # plot the filter\n",
    "        ax[0,5].axis('off') # turn off the axis\n",
    "        ax[1,5].axis('off') # turn off the axis\n",
    "        plt.show() # show the plot\n",
    "\n",
    "plot_convolution(torch.tensor([[-1.,0.,1.],[-1.,0.,1.],[-1.,0.,1.]]), 'Vertical edge filter')\n",
    "plot_convolution(torch.tensor([[-1.,-1.,-1.],[0.,0.,0.],[1.,1.,1.]]), 'Horizontal edge filter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First fileter is called a `vertical edge filter`, and it is defined by the following matrix:\n",
    "\n",
    "$$\n",
    "\\left(\\begin{matrix}\n",
    "-1 & 0 & 1 \\\\\n",
    "-1 & 0 & 1 \\\\\n",
    "-1 & 0 & 1 \\\\\n",
    "\\end{matrix}\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "When this filter goes over realatively uniform pixel field, all values add up to 0. However, when it encounters a vertical edge in the image, high spike value is generated. That's why in the images above you can see vertical edges represented by high and low values, while horizontal edges are averaged out.\n",
    "\n",
    "An oposite thing happens when we apply horizontal edge filter - horizontal lines are amplified, and vertical are averaged out.\n",
    "\n",
    "In classical computer vision, **multiple filters were applied to the image to generate features, which then were used by machine learning algorithm to build a classifier**. However, in `deep learning` we construct networks that `learn` best convolutional filters to sovle classfication problem.\n",
    "\n",
    "To do that, we introduce `convolutional layers`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Covolutional layers\n",
    "\n",
    "Convolutional layers are defined using `nn.Conv2d` construction. We need to specify the following:\n",
    "\n",
    "* `in_channels` - number of channels in the input image. For example, MNIST images are grayscale, so we have only one channel. If we were using color images, we would have 3 channels - red, green and blue.\n",
    "* `out_channels` - number of channels produced by the convolution. This is equal to the number of filters we want to use.\n",
    "* `kernel_size` - size of the convolutional filter(sliding window). For example, if we have `kernel_size=3`, then each convolutional filter will be 3x3. Usually 3x3 or 5x5 filters are used.\n",
    "\n",
    "Simplest CNN will contain one convolutional layer. Given the input size 28x28, after applying nine 5x5 filters we will end up with a tensor of 9x24x24 (the spatial size is smaller, because there are only 24 positions where a sliding interval of length 5 can fit into 28 pixels).\n",
    "\n",
    "After convolution, we flattern 9x24x24 tensor into one vector of size 5184, and then add linear layer, to produce 10 classes. We also use `ReLU` activation function in between layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneConv(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(OneConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(kernel_size=(5,5), out_channels=9, in_channels=1) # create a convolution layer\n",
    "        self.flatten = nn.Flatten() # nn.Flattern is used to flatten the output of the convolution layer to a vector\n",
    "        self.fc = nn.Linear((9*24*24), 10) # create a fully connected layer\n",
    "    def forward(self, x):\n",
    "        x = nn.functional.relu(self.conv(x)) # nn.funtional is used to apply the relu function\n",
    "        x = self.flatten(x) # flatten the output of the convolution layer\n",
    "        x = nn.functional.log_softmax(self.fc(x), dim=1) # apply the log_softmax function to the output of the fully connected layer, more https://www.kaggle.com/code/aisuko/building-the-model-layers?scriptVersionId=136624674&cellId=28\n",
    "        return x\n",
    "\n",
    "net=OneConv().to(torch_device)\n",
    "summary(net, input_size=(1,1,28,28))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that this network contains around 50k trainable parameters, compared to around 80k in fully-connected multi-layered networks. This allows us to achieve good results even on smaller datasets, because convolutional networks generalize much better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(net, dataloader, lr=0.01, optimizer=None, loss_fn=nn.NLLLoss()):\n",
    "    optimizer = optimizer or torch.optim.Adam(net.parameters(), lr=lr) # use Adam optimizer if not provided\n",
    "    net.train() # put the network into training mode make sure the parameters are trainable\n",
    "    total_loss,acc,count =0,0,0\n",
    "    for features, labels in dataloader:\n",
    "        features,labels = features.to(torch_device), labels.to(torch_device)\n",
    "        optimizer.zero_grad() # reset the gradients to zero before each batch to avoid accumulation\n",
    "        out=net(features) # forward pass of the mini-batch through the network to obtain the outputs\n",
    "        loss=loss_fn(out,labels) # compute the loss\n",
    "        loss.backward() # compute the gradients of the loss with respect to all the parameters of the network\n",
    "        optimizer.step() # update the parameters of the network using the gradients to minimize the loss\n",
    "        total_loss+=loss # accumulate the loss for inspection\n",
    "        _,preds=torch.max(out,dim=1) # compute the predictions to obtain the accuracy\n",
    "        acc+=(preds==labels).sum() # accumulate the correct predictions\n",
    "        count+=len(labels) # accumulate the total number of examples\n",
    "    return total_loss.item()/count, acc.item()/count # return the loss and accuracy\n",
    "\n",
    "def validate(net, dataloader, loss_fn=nn.NLLLoss()):\n",
    "    net.eval() # put the network into evaluation mode to deactivate the dropout layers\n",
    "    count,acc,loss =0,0,0\n",
    "    with torch.no_grad(): # deactivate autograd to save memory and speed up computations\n",
    "        for features, labels in dataloader:\n",
    "            features,labels = features.to(torch_device), labels.to(torch_device)\n",
    "            out=net(features) # forward pass of the mini-batch through the network to obtain the outputs\n",
    "            loss += loss_fn(out,labels) # compute the loss\n",
    "            preds=torch.max(out,dim=1)[1] # compute the predictions to obtain the accuracy\n",
    "            acc+=(preds==labels).sum() # accumulate the correct predictions\n",
    "            count+=len(labels) # accumulate the total number of examples\n",
    "    return loss.item()/count, acc.item()/count # return the loss and accuracy\n",
    "\n",
    "def train(net, train_loader, test_loader, optimizer=None, lr=0.01, epochs=10, loss_fn=nn.NLLLoss()):\n",
    "    optimizer = optimizer or torch.optim.Adam(net.parameters(), lr=lr) # use Adam optimizer if not provided\n",
    "    res = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc':[]}\n",
    "    for ep in range(epochs):\n",
    "        tl, ta = train_epoch(net, train_loader, optimizer=optimizer,lr=lr, loss_fn=loss_fn)\n",
    "        vl,va = validate(net, test_loader, loss_fn=loss_fn)\n",
    "        print(f\"Epoch {ep:2}, Train acc={ta:.3f}, Val acc={va:.3f}, Train loss={tl:.3f}, Val loss={vl:.3f}\")\n",
    "        res['train_loss'].append(tl)\n",
    "        res['train_acc'].append(ta)\n",
    "        res['val_loss'].append(vl)\n",
    "        res['val_acc'].append(va)\n",
    "    return res\n",
    "\n",
    "def plot_results(hist):\n",
    "    plt.figure(figsize=(15,5)) # figure is used to create a new figure\n",
    "    plt.subplot(121) # subplot is used to create a new subplot on a grid\n",
    "    plt.plot(hist['train_acc'], label='Training acc')\n",
    "    plt.plot(hist['val_acc'], label='Validation acc')\n",
    "    plt.legend() # legend is used to add a legend to the plot\n",
    "    plt.subplot(122) # plot the loss\n",
    "    plt.plot(hist['train_loss'], label='Training loss')\n",
    "    plt.plot(hist['val_loss'], label='Validation loss')\n",
    "    plt.legend()\n",
    "\n",
    "hist = train(net, train_loader, test_loader, epochs=5)\n",
    "plot_results(hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we are able to achieve higher accuracy, and much faster, compared to the fully-connected networks. We can also visualize the weights of our trained convolutional layers, to try and make some more sense of what is going on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,9)\n",
    "with torch.no_grad():\n",
    "    p = next(net.conv.parameters())\n",
    "    for i,x in enumerate(p):\n",
    "        ax[i].imshow(x.detach().cpu()[0,...])\n",
    "        ax[i].axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that some of those filters look like they can recognize some oblique strokes, while others loook pretty random."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-layered CNNs and pooling layers\n",
    "\n",
    "First converlutional layers looks for primitive patterns, such as horizontal or vertical lines, but we can apply further convolutional layers on top of them to look for higher-level patterns, such as primitive shapes. Then more convolutional layers can combine those shapes into some parts of the picture, up to the final object that we are trying to classsify.\n",
    "\n",
    "When doing so, we may also apply one trick: reducing the spatial size of the image. Once we have detected there is a horizontal stoke within sliding 3x3 window, it is not so important at which exact pixel it occurred. Thus we can \"scale down\" the size of the image, which is done using one of the `pooling layers`:\n",
    "\n",
    "* `Average Pooling` takes a sliding window (for example, 2x2 pixels) and computes an average of valyes within the window\n",
    "* `Max Pooling` replaces the window with the maximum value. The idea behind max pooling is to detect a presence of a certain pattern within the sliding window.\n",
    "\n",
    "Thus, in a typical CNN there would be several convolutional layers, with pooling layers in between them to decrease dimensions of the image. We would also increase the number of filters, because as patterns become more advanced  - there are more possible interesting combinations that we need to be looking for.\n",
    "\n",
    "<figure><img src=\"https://hostux.social/system/media_attachments/files/110/729/082/002/900/524/original/1c7f7d5fc1cc2373.png\" alt=\"\" ><figcaption><p>Source from Microsoft learning</p></figcaption></figure>\n",
    "\n",
    "Because of decreasing spatial dimensions and increasing feature/filters dimensions, this architecture is also called `pyramid architecture`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerCNN(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(MultiLayerCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1,10,5) # 1 input channel, 10 output channels, 5x5 filter\n",
    "        self.pool = nn.MaxPool2d(2,2) # 2x2 max pooling\n",
    "        self.conv2 = nn.Conv2d(10,20,5) # 10 input channels, 20 output channels, 5x5 filter\n",
    "        self.fc = nn.Linear(20*4*4,10) # 320 input features, 10 output features\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(nn.functional.relu(self.conv1(x))) # apply the first convolution, relu and pooling\n",
    "        x = self.pool(nn.functional.relu(self.conv2(x))) # apply the second convolution, relu and pooling\n",
    "        x = x.view(-1, 320) # view is used to reshape the tensor, -1 means the size is inferred from the other dimensions, 320 is the number of input features for the fully connected layer\n",
    "        x = nn.functional.log_softmax(self.fc(x), dim=1) # apply the log_softmax function to the output of the fully connected layer, more https://www.kaggle.com/code/aisuko/building-the-model-layers?scriptVersionId=136624674&cellId=28\n",
    "        return x\n",
    "\n",
    "net = MultiLayerCNN().to(torch_device)\n",
    "summary(net, input_size=(1,1,28,28)) # summary is used to print a summary of the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are few things about this definition:\n",
    "* Instead of using `Flatten` layer, we are flattening the tensor inside `forward` function using `view` function. Since flattening layer does not have trainable weights, it is not essential that we create a separate layer instance within our class.\n",
    "* We use just one instance of pooling layer in our model, also because it does not contain any trainable parameters, and this one instance can be effectively resued.\n",
    "* The number of trainable parameters (~8.5k) is dramatically smaller than in previous cases. This happens becuasee convolutional layers in general have few parameters, and dimentionality of the image before applying final dense layer is significantly reduced. Small number of parameters have positive impact on our models, because it helps to prevent overfitting even on smaller dataset sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = train(net, train_loader, test_loader, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What you should probably observe is that we are able to achieve higher accuracy than with just one layer, and much faster -  just with 1 or 2 epochs. It means that sophisticated network architecture needs much fewer data to figure out what is going on, and to extract generic patterns from our images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playing with real images from the CIFAR-10 dataset\n",
    "\n",
    "While our handwritten digit recognition problem may seem like a toy problem, we are now ready to do something more serious. Let's explore more advanced dataset of pictures of different object, called [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html). It contains 60k 32x32 images, divided into 10 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=14, shuffle=True) # shuffle is used to shuffle the data before each epoch in order to avoid overfitting\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=14, shuffle=False)\n",
    "\n",
    "classses = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck') # the classes is used to map the labels to the names of the classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_dataset(dataset, n=10, classes=None):\n",
    "    fig, ax=plt.subplots(1,n,figsize=(15,3))\n",
    "    mn = min(dataset[i][0].min() for i in range(n))\n",
    "    mx = max(dataset[i][0].max() for i in range(n))\n",
    "\n",
    "    for i in range(n):\n",
    "        ax[i].imshow(np.transpose((dataset[i][0]-mn)/(mx-mn), (1,2,0))) # transpose is used to change the order of the dimensions in order to plot the image\n",
    "        ax[i].axis('off')\n",
    "        if classes:\n",
    "            ax[i].set_title(classes[dataset[i][1]])\n",
    "\n",
    "display_dataset(trainset, classes=classses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The architecture for CIFAR-10 is called LeNet, and has been proposed by Yann LeCun. It follows the same principles as we have outlined above, the main difference being 3 inpput color channels instead of 1.\n",
    "\n",
    "We also do one more simplification to this model - we do not use `log_softmax` as output activation function, and just return the output of last fully-connected layer. In this case we can just use `CrossEntropyLoss` loss function to optimize the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3,6,5) # 3 input channel, 6 output channels, 5x5 filter\n",
    "        self.pool = nn.MaxPool2d(2,2) # 2x2 max pooling\n",
    "        self.conv2 = nn.Conv2d(6,16,5) # 6 input channels, 16 output channels, 5x5 filter\n",
    "        self.conv3 = nn.Conv2d(16,120,5) # 16 input channels, 120 output channels, 5x5 filter\n",
    "        self.flat = nn.Flatten() # flatten the output of the convolution layer to a vector\n",
    "        self.fc1 = nn.Linear(120,64) # 120 input features, 64 output features\n",
    "        self.fc2 = nn.Linear(64,10) # 64 input features, 10 output features\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(nn.functional.relu(self.conv1(x))) # apply the first convolution, relu and pooling\n",
    "        x = self.pool(nn.functional.relu(self.conv2(x))) # apply the second convolution, relu and pooling\n",
    "        x = nn.functional.relu(self.conv3(x)) # apply the third convolution and relu\n",
    "        x = self.flat(x) # flatten the output of the convolution layer\n",
    "        x = nn.functional.relu(self.fc1(x)) # apply the first fully connected layer and relu\n",
    "        x = self.fc2(x) # apply the second fully connected layer\n",
    "        return x\n",
    "\n",
    "net = LeNet().to(torch_device)\n",
    "summary(net, input_size=(1,3,32,32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training this network properly will take significant amount of time, aand should preferably be done on GPU-enable compute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.SGD(net.parameters(), lr=0.01, momentum=0.9) # use SGD optimizer with momentum\n",
    "hist = train(net, train_loader, test_loader, epochs=5, optimizer=opt, loss_fn=nn.CrossEntropyLoss()) # use CrossEntropyLoss as loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy that we have been able to achieve with 3 epochs of training does not see, great. However, remember that blind guessing would only give us 10% accuracy, and that our problem is actually significantly more difficult than MNIST digit classification. Getting above 50% accuracy in such a short training time seems like a good accomplishment.\n",
    "\n",
    "Real-life architectures that power image classification, object detection, and even image generation networks are all based on CNNs, just with more layers and some additional tricks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions for review\n",
    "\n",
    "#### If the size of an input image is 3x200x200, what would be the size of the tensor after applying a 5x5 convolutional layer with 16 filters?\n",
    "\n",
    "To calcualte the size of the tensor after applying a 5x5 convolutional layer with 16 filters to an input image of size 3x200x200, we can use the formula:\n",
    "\n",
    "$$Output size = (Input size - Filter size + 2 * Padding)/Stride +1$$\n",
    "\n",
    "Given that the input image size is 3x200x200 and the filter size is 5x5, we can calcualte the output size as follows:\n",
    "* Input size: 3x200x200\n",
    "* Filter size: 5x5\n",
    "* Padding: 0 (assuming no padding)\n",
    "* Stride: 1 (assuming stride of 1)\n",
    "\n",
    "For the first dimension (number of channels), the output size will be the same as the number of filters, which is 16.\n",
    "\n",
    "For the second and third dimensions (height and width), we can use the formula:\n",
    "\n",
    "$$Output size = (200 - 5 + 2 * 0)/1 +1 = 196$$\n",
    "\n",
    "Therefore, the size of the tensor after applying a 5x5 convolutional layer with 16 filters to an input image of size 3x200x200 would be 16x196x196."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
