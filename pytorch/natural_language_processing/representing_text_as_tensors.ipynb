{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representing text\n",
    "\n",
    "If we want ot solve NLP tasks with neural networks, we need some way to represent text as tensors. Computers already represent textual characters as numbers that map to fonts on your screen using encodings such ASCII or UTF-8.\n",
    "\n",
    "<figure><img src=\"https://hostux.social/system/media_attachments/files/110/744/867/966/745/527/original/2da950ca853de826.png\" alt=\"\" width=\"1000\"><figcaption><p>Source from Microsoft Learning </a> </p></figcaption></figure>\n",
    "\n",
    "We understand what each letter **represents**, and how all characters come topgether to form the words of a sentence. However, computers by themeselves do not have such an understanding, and neural network has to learn the meaning during training.\n",
    "\n",
    "Therefore, we can use different approaches when representing text:\n",
    "\n",
    "* **Character-level representation**, when we represent text by treating each character as a number. Given that we have $C$ different characters in out text corpus, the world Hello would be represented by $5*C$ tensor. Each letter would correspond to a tensor column in one-hot encoding.\n",
    "* **Word-level representation**, in which we create a **vocabulary** of all words in our text, and then represent words using one-hot encoding, This approach is somehow better, because each letter by itself does not have much meaning, and thus by using higher-level semantic concepts - words - we simplify the task for the neural network. However, given large dictionary size, we need to deal with high-dimentional sparse tensors.\n",
    "\n",
    "\n",
    "TO unify those approaches, we typically call an atomic piece of text a **a token**. In some cases tokens can be letters, in order cases - words, or parts of words.\n",
    "\n",
    "> For example, we can choose to tokenize `indivisible` as `in -divis -ible`, where the # sign represents that the token is a continuation of the previous word. This would allow the root `divis` to always be reperesented by one token, corresponding to one core meaning.\n",
    "\n",
    "The process of converting text into a sequence of tokens is called **tokenization**. Next, we need to assign each token to a number, which we can feed into a neural network. This is called **vectorization**, and is normally done by building a token vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install portalocker>=2.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text classification task\n",
    "\n",
    "We will start with a simple text classification tasks based on **AG_NEWS** dataset, which is to classify news headliness into one of categories: World, Sports, Business or Sci/Tech. This dataset is built into `TorchText`, and we can easily download it by using `torchtext.datasets.AG_NEWS` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "import os\n",
    "import collections\n",
    "\n",
    "os.makedirs('./data', exist_ok=True)\n",
    "train_dataset, test_dataset = torchtext.datasets.AG_NEWS(root='./data')\n",
    "classes = ['World', 'Sports', 'Business', 'Sci/Tech']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the paltform, Apple Silicon or Linux\n",
    "import os, platform\n",
    "\n",
    "torch_device = 'mps' if platform.system() == 'Darwin' else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, `train_dataset` and `test_dataset` contain iterators that return pairs of label (number of class) and text respectively, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print out the first 5 new headlines from our datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,x in zip(range(5), train_dataset):\n",
    "    print(f'**{classes[x[0]]}** -> {x[1]}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because datasets are iterators if we want to use the data multiple times we need to convert it to list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = torchtext.datasets.AG_NEWS(root='./data')\n",
    "train_dataset = list(train_dataset)\n",
    "test_dataset = list(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization and Vectorization\n",
    "\n",
    "Now we need to convert text into `numbers` that can be represented as tensors. If we want word-level representation, we need to do two things:\n",
    "* First step is convert text to tokens(`tokenization`)\n",
    "* build a `vocabulary` of those tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = torchtext.data.utils.get_tokenizer('basic_english')\n",
    "first_sentence = train_dataset[0][1]\n",
    "second_sentence = train_dataset[1][1]\n",
    "\n",
    "f_tokens = tokenizer(first_sentence)\n",
    "s_tokens = tokenizer(second_sentence)\n",
    "\n",
    "print(f'\\nfirst token list: \\n{f_tokens}')\n",
    "print(f'\\nsecond token list: \\n{s_tokens}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, to convert text to numbers, we will need to build a vocabulary of all tokens. We first build the dictionary using the `Counter` objet, and then create a `Vocab` object that would help us deal with vectorization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "\n",
    "counter =  collections.Counter()\n",
    "for (label, line) in train_dataset:\n",
    "    counter.update(tokenizer(line))\n",
    "\n",
    "vocab = torchtext.vocab.vocab(counter, min_freq=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see how each word maps to the vocabulary, we'll loop through each word in the list to lookup it's index number in `vocab`. Each word or character is displayed with it's corresponding index. For example, word `the` appears several times in both sentence and it's unique index in the vocab is the number 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_lookup_f = [list((vocab[w],w) for w in f_tokens)]\n",
    "print(f'\\nIndex lockup in 1st sentence:\\n{word_lookup_f}')\n",
    "\n",
    "word_lookup_s = [list((vocab[w],w) for w in s_tokens)]\n",
    "print(f'\\nIndex lockup in 2nd sentence:\\n{word_lookup_s}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using vocabulary, we can easily encode out tokenized string into a set of numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "print(f'Vocab size: {vocab_size}')\n",
    "\n",
    "def encode(text):\n",
    "    return [vocab.get_stoi()[s] for s in tokenizer(text)]\n",
    "\n",
    "vec=encode(train_dataset[0][1])\n",
    "print(vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The torchtext `vocab.get_stoi` function allows us to convert from string representation into numbers (the name stoi stands for string-to-integers). To convert the text back from a numeric representation into text, we can use the `vocab.get_itos` dictionary to perform reverse lookup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(text):\n",
    "    return [vocab.get_itos()[i] for i in text]\n",
    "\n",
    "print(decode(vec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words text representation\n",
    "\n",
    "Because words represent meaning, sometimes we can figure out the meaning of a text by just looking at the individual words, regardless of their order in the sentence. For example, when classifying news, words like `weather`, `snow` are likely to indicate `weather forecast`, while words like `stocks`, `dollar` would count towards `financial news`.\n",
    "\n",
    "**Bag of Words** (BoW) vector representation is the most commonly used traditional vector representation. Each word is linked to a vector index, vector element contains the number of occureences of a word in a given document.\n",
    "\n",
    "<figure><img src=\"https://hostux.social/system/media_attachments/files/110/745/067/186/920/286/original/18c9a8e5cd6c7dfb.png\" alt=\"\" width=\"1000\"><figcaption><p>Source from Microsoft Learning </a> </p></figcaption></figure>\n",
    "\n",
    "> Note: You can also think of BoW as a sum of all one-hot-encoded bectors for individual words in the text.\n",
    "\n",
    "Below is an example of how to generate a bag od word representation using the Scikit Learn python library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "corpus = [\n",
    "    'I like hot dogs.',\n",
    "    'The dog ran fast.',\n",
    "    'Its hot outside.',\n",
    "]\n",
    "\n",
    "vectorizer.fit_transform(corpus)\n",
    "vectorizer.transform(['My dog likes hot dogs on a hot day.']).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute bag-of-words vector form the vector representation of our AG_NEWS dataset, we can use the following function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: Here we are using global `vocab_size` variable to specify default size of the vocabulary. Since often vocabulary size id pretty big, we can limit the size of the vocabulary to most frequent words. Try lowering `vocab_size` value and running the code below, and see how it affects the accuracy. You should expect some accuracy drop, but not dramatic, in lieu of higher performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "\n",
    "def to_bow(text, bow_vocab_size=vocab_size):\n",
    "    \"\"\"\n",
    "    Convert text string to a bag-of-words tensor.\n",
    "    \"\"\"\n",
    "    res = torch.zeros(bow_vocab_size, dtype=torch.float32)\n",
    "    for i in encode(text):\n",
    "        if i <bow_vocab_size:\n",
    "            res[i] += 1\n",
    "    return res\n",
    "\n",
    "print(f'sample text:{train_dataset[0][1]}')\n",
    "print(f'bow vector: {to_bow(train_dataset[0][1])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training BoW classifier\n",
    "\n",
    "Now that we have learned how to build Bag-of-Words representation of our text, let's train a classifier on top of it.\n",
    "\n",
    "First, we need to convert our dataset for training in such a way, that all positional vector representations are converted to bag-of-words representation. This can be achieved by passing `bowify` function as `collate_fn` parameter to standard torch `DataLoader`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# This collate function gets list of batch_size tuples, and needs to return a pair of label-feature tensors for the whole minibatch\n",
    "def bowify(b):\n",
    "    return(\n",
    "        torch.LongTensor([t[0]-1 for t in b]),\n",
    "        torch.stack([to_bow(t[1]) for t in b])\n",
    "    )\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=bowify) # collate_fn is the function that merges the list of samples into a mini-batch\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=True, collate_fn=bowify)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define simple classifier neural network that contains one linear layer. The size of the input vector equals to `vocab_size`, and output size coresponds to the number of classes(4). Because we are solving classification task, the final activation function is `LogSoftmax`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = torch.nn.Sequential(\n",
    "    torch.nn.Linear(vocab_size, 4),\n",
    "    torch.nn.LogSoftmax(dim=1)\n",
    ")\n",
    "\n",
    "net=net.to(torch_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will define standard PyTorch training loop. Because our dataset is quite large, we will train only for one epoch, and sometimes even for less than an epoch(specifying the `epoch_size` parameter allows us to limit training). We would also report accumulated training accuracy during training; the frequency of reporting is pecified using `report_freq` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eopch(net, dataloader, lr=0.01, optimizer=None, loss_fn=torch.nn.NLLLoss(), epoch_size=None, report_freq=200):\n",
    "    optimizer = optimizer or torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    net.train()\n",
    "    total_loss, acc, count,i = 0,0,0,0\n",
    "    for labels, features in dataloader:\n",
    "        labels, features = labels.to(torch_device), features.to(torch_device)\n",
    "        optimizer.zero_grad()\n",
    "        output = net(features)\n",
    "        loss = loss_fn(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        acc += (predicted == labels).sum()\n",
    "        count += len(labels)\n",
    "        i+=1\n",
    "        if i%report_freq == 0:\n",
    "            print(f'iteration {count}, loss {total_loss.item()/count}, accuracy {acc.item()/count}') # item() is used to get the value of a tensor\n",
    "        if epoch_size  and count >= epoch_size:\n",
    "            break\n",
    "    \n",
    "    return total_loss.item()/count, acc.item()/count\n",
    "\n",
    "train_eopch(net, train_loader, epoch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BiGrams, TriGrams and N-Grams\n",
    "\n",
    "One limitation of a bag of words approach is that some words are part of multi word expressions, for example, the word `hot dog` has a completely different meaning than the words `hot` and `dog` in other contexts. If we represent words `hot` and `dog` always by the same vectors, it can confuse our model.\n",
    "\n",
    "To address this, **N-gram representations** are often used in methods of document classification, where the frequency of each word, **bi-word** ot **tri-word** is useful feature for training classifiers. \n",
    "\n",
    "* In bigram representaion, for example, we will add all word pairs to the vocabulary, in addition to original words.\n",
    "\n",
    "Below is an example of how to generate a bigram bag of word representation using the Scikit Learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_vectorizer = CountVectorizer(ngram_range=(1,2), token_pattern=r'\\b\\w+\\b', min_df=1)\n",
    "corpus = [\n",
    "    'I like hot dogs.',\n",
    "    'The dog ran fast.',\n",
    "    'Its hot outside.',\n",
    "]\n",
    "\n",
    "bigram_vectorizer.fit_transform(corpus)\n",
    "print(\"Vocabulary: \", bigram_vectorizer.vocabulary_)\n",
    "bigram_vectorizer.transform(['My dog likes hot dogs on a hot day.']).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **main drawback of N-gram approach** is that vocabulary size starts to grow exremly fast. In practice, we need to combine N-gram representation with some dimensionality reduction technique, such as embeddings, which we will discuss in the next notebook.\n",
    "\n",
    "To use N-gram representaion in our **AG News** dataset, we need to build special ngram vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = collections.Counter()\n",
    "for (label, line) in train_dataset:\n",
    "    l = tokenizer(line)\n",
    "    counter.update(torchtext.data.utils.ngrams_iterator(l, ngrams=2))\n",
    "\n",
    "bi_vocab = torchtext.vocab.vocab(counter, min_freq=1)\n",
    "\n",
    "print(f'Bigram vocabulary length: {len(bi_vocab)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could then use the same code as above to train the classifier, however, it would be very memory-inefficient. In the next notebook, we will train bigram classifier using embeedings.\n",
    "\n",
    ">Note: We can only leave those ngrams that occur in the text more than specified number of times. This will make sure that infrequent bigrams will be omitted, and will decrease the dimensionality significantly. To do this, set `min_freq` parameter to a higher value, and observe the length of vocabulary change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term Frequency Inverse Document Frequency TF-IDF\n",
    "\n",
    "In BoW representaion, word occurences are evenly weighted, regardless of the word iteself. However, it is clear that frequent words, such as a,in, etc. are much less important for the classification, than specialized terms. In fact, in most NLP tasks some words are more relevant than others.\n",
    "\n",
    "**TF-IDF** stands for **term frequency-inverse document frequency**. It is a variation of ag of words, where instead of a binary 0/1 value indicating the appearence of a word in a focument, a floating-point value is used, which is related to the frequency of word occurrence in the corpus.\n",
    "\n",
    "More formally, the weights $w_{ij}$ of a word $i$ in the document $j$ is defined as:\n",
    "\n",
    "$$w_{ij} = tf_{ij} \\times \\log({N \\over df_i})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where\n",
    "* i is the word\n",
    "* j is the document\n",
    "* $w_{ij}$ is the weight or the importance of the word in the document\n",
    "* $tf_{ij}$ is the number of occurences of word $i$ in the document $j$. i.e. the BoW value we have seen before\n",
    "* $N$ is the number of documents in the collection\n",
    "* $df_i$ is the number of documents containing the word $i$ in the whole collection\n",
    "\n",
    "TF-IDF value $w{ij}$ increases proportionally to the number of times a word appears in the document and is offset by the number of documents in the corpus that contains the word, which helps to adjust for the fact that some words appear more frequently than others. For example, if the word appeears in every document in the collection, $df_{i}=N$, and $w_{ij} = 0$, and those terms would be completely disregarded.\n",
    "\n",
    "We can easily create TF-IDF vectorization of text using Scikit Learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,2))\n",
    "vectorizer.fit_transform(corpus)\n",
    "vectorizer.transform(['My dog likes hot dogs on a hot day.']).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can implement it ourselves. First, let's compute document frequency $df_{i}$ for each word $i$. We can represent it as tensor of size `vocab_size`. We will limit the number of documents to `N=1000` to speed up processing. For each input sentence, we compute the set of words(represented by their numbers), and increase the corresponding counter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5 # small value for testing the whole process\n",
    "df = torch.zeros(vocab_size)\n",
    "for _,line, in train_dataset[:N]:\n",
    "    for i in set(encode(line)):\n",
    "        df[i] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have document frequencies for each word, we can define `tf_idr` function that will take a string, and produce TF-IDF vector. We will use `to_bow` defined above to calculate term frequency vector, and multiply it by inverse document frequency of the corresponding term. Remeber that all tensor operations are element-wide, which allows us to implement the whole computation as a tensof formula:\n",
    "\n",
    "> Here we use $\\log({N+1\\over df_i+1})$ instead of $\\log({N\\over df_i})$. This yields simiar results, but prevents division by 0 in those cases when $df_i=0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf(s):\n",
    "    bow=to_bow(s)\n",
    "    tf = bow*torch.log((N+1)/(df+1))\n",
    "\n",
    "print(tf_idf(train_dataset[0][1])) # due to small N, the result `None` is not correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However even though TF-IDF representaions provide frequency weight to different words they are unable to represent meaing or order. As the famous linguist J.R. Firth said in 1935, \"The complete meaning of a word is always contextual, and no study of meaning apart from context can be taken seriously.\". We will learn in the later notebooks how to capture contextual information from text using language modeling."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
