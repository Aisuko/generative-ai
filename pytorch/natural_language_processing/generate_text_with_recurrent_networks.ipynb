{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative networks\n",
    "\n",
    "Recurrent Neural Networks (RNNs) and their gated cell variants such as Long Short Term Memory Cells (LSTMs) and Gated Recurrent Units (GRUs) provided a mechanism for language modeling, i.e. they can leanr word ordering and provide predictions for next word in a sequence. This allows us to use RNNs for **generative tasks**, such as ordinary text generation, machine translation, and even image captionaing.\n",
    "\n",
    "In RNN architecture, each RNN unit produced next next hidden state as an output. However, we can also add another output to each recurrent unit, which would allow us to output s **sequence** (which is equal in length to the original sequence). Moreover, we can use RNN units that do not accept an input at each step, and just take some initial state vector, and then produce a sequence of outputs.\n",
    "\n",
    "This allows for different neural architectures that are shown in the picture below:\n",
    "\n",
    "<figure><img src=\"https://hostux.social/system/media_attachments/files/110/768/729/925/794/877/original/fa0c21dbd618dfde.jpg\" alt=\"\" width=\"1000\"><figcaption><p>Source from Unreasonable Effectiveness of Recurrent Neural Networksn by Andrej Karpaty </a> </p></figcaption></figure>\n",
    "\n",
    "* **One-to-one** is a traditional neural network with one input and one output.\n",
    "* **One-to-many** is a generative a architecure that accepts on einput value, and generates a sequence of output values. For example, if we want to train `image caotioning` network that would produce a textual description of a picture. we can have a picture as input, pass it through CNN to obtain hidden state, and then have recurrent chain generate caption word-by-word\n",
    "* **Many-to-one** corrsponds to RNN architectures we described in the `Capture patterns with recurrent neural networks`, such as text classification\n",
    "* **Many-to-many** or **sequence-to-sequence** corresponds to tasks such as `machine translation`, where we have first RNN collect all information form the input sequence into the hidden state, and another RNN chain unrolls this state into the output sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will focus on simple generative models that will help us generate text. For simplicity, let's build **character-level network**, which generates text letter by letter. During training, we need to take some text corpus, and split it into letter sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/pytorch/data/issues/1093\n",
    "pip install portalocker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu118"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building character vocabulary\n",
    "\n",
    "To build character-level generative network, we need to split text into individual characters instead of words. This can be done by defining a different tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "import numpy as np\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading dataset\n",
    "train_dataset, test_dataset = torchtext.datasets.AG_NEWS(root='./data')\n",
    "train_dataset, test_dataset = list(train_dataset), list(test_dataset)\n",
    "classes = ['World','Sports','Business','Sci/Tech']\n",
    "\n",
    "tokenizer = torchtext.data.utils.get_tokenizer('basic_english')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def char_tokenizer(words):\n",
    "    return list(words)\n",
    "\n",
    "counter = collections.Counter()\n",
    "for (label, line) in train_dataset:\n",
    "    counter.update(char_tokenizer(line))\n",
    "vocab = torchtext.vocab.vocab(counter)\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "print(vocab_size)\n",
    "print(vocab.get_stoi()['a'])\n",
    "print(vocab.get_itos()[13])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the example of how we can encode the text from out dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(x, voc=None,tokenizer=tokenizer):\n",
    "    v =vocab if not voc else voc\n",
    "    return [v.get_stoi()[s] for s in tokenizer(x)]\n",
    "\n",
    "\n",
    "def enc(x):\n",
    "    return torch.LongTensor(encode(x, voc=vocab, tokenizer=char_tokenizer))\n",
    "\n",
    "print(train_dataset[0][1])\n",
    "print(enc(train_dataset[0][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a generative RNN\n",
    "\n",
    "The way we will trian RNN to generate text is the following. On each step, we will take a sequence of characters of length `nchars`, and ask the networks to generate the next output character for each input character:\n",
    "\n",
    "<figure><img src=\"https://hostux.social/system/media_attachments/files/110/768/908/937/200/134/original/d8b268cc82ca6080.png\" alt=\"\" width=\"1000\"><figcaption><p>Source from MicrosoftLearning </a> </p></figcaption></figure>\n",
    "\n",
    "Depending on the actual scenario, we may also want to inlcude some special characters, such as `end-of-sequence` `<eos>`. In our case, we just want to train the network for endless text generation, thus we will fix the size of each sequence to be equal to `nchars` tokens. Consequently, each training example will consist of `nchars` inputs and `nchars` outputs(which are input sequence shifted one symbol to the left). Minibatch will consist of several such sequences.\n",
    "\n",
    "The way we will generate minibatches is to take each news text of length `l`, and generate all possible input-output combinations from it (there will be `l-nchars` such combinations). They will from one minibatch, and size of minibatches would be different at each training step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the paltform, Apple Silicon or Linux\n",
    "import os, platform\n",
    "\n",
    "torch_device=\"cpu\"\n",
    "\n",
    "if 'kaggle' in os.environ.get('KAGGLE_URL_BASE','localhost'):\n",
    "    torch_device = 'cuda'\n",
    "else:\n",
    "    torch_device = 'mps' if platform.system() == 'Darwin' else 'cpu'\n",
    "\n",
    "torch_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nchars= 100\n",
    "\n",
    "def get_batch(s, nchars=nchars):\n",
    "    ins = torch.zeros(len(s)-nchars, nchars, dtype=torch.long, device=torch_device)\n",
    "    outs = torch.zeros(len(s)-nchars, dtype=torch.long, device=torch_device)\n",
    "\n",
    "    for i in range(len(s)-nchars):\n",
    "        ins[i]=enc(s[i:i+nchars])\n",
    "        outs[i]=enc(s[i+1:i+nchars+1])\n",
    "    return ins, outs\n",
    "\n",
    "get_batch(train_dataset[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's define generator network. It can be based on any recurrent cell which we discussed in the previous notebooks(simple, LSTM ot GRU). In our example we will use LSTM.\n",
    "\n",
    "Because the network takes characters as inputs, and vocabulary size is pretty small, we do not need embedding layer, one-hot-encoded input can directly go to LSTM cell. However, because we pass character numbers as input, we need to one-hot-encode them before passing to LSTM. This is done by calling `one_hot` function during `forward` pass. Output encoder would be a linear layer that will conver hiddent state into one-hot-encoded output.\n",
    "\n",
    ">Note: One-hot-encoding involves representing each character as a binary vector, where only the index corrsponding to the character's value is set to 1, and all other indices are set to 0. This encoding allows the LSTM to process the characters as input and learn patterns from them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMGenerator(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.nn = torch.nn.LSTM(vocab_size, hidden_dim, batch_first=True)\n",
    "        self.fc = torch.nn.Linear(hidden_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, x, s=None):\n",
    "        x = torch.nn.functional.one_hot(x, num_classes=vocab_size).to(torch.float32)\n",
    "        x,s = self.rnn(x,s)\n",
    "        return self.fc(x),s"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
