{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data visualization and transformation is an important part of every model. Now that we have our dataset downloaded, let's learn more about audio data visualization and transforming this dataset.\n",
    "\n",
    "TorchAudio has many transformation functions for audio manipulation and feature extractions. However, we will focus on the following concepts and tranforms:\n",
    "* **Spectrogram** Create a spectrogram from a waveform.\n",
    "* **MelSpectrogram** Create Mel Spectrogram from a waveform using the `STFT` function in PyTorch.\n",
    "* **Waveform**\n",
    "* **MFCC** Create the Mel-frequency cepstrum coefficients from a waveform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll go through the audio file tthat we downloaded in local directory by filtering out the ones that are the `yes` abd `no` commands under the `nohash` path. Then we'll load the files into the `torchaudio` data object. This will make it easy to extract attributes of the audio (for example, the waveform and sample rate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio_file(path:str, label:str):\n",
    "    dataset =[]\n",
    "    walker = sorted(str(p) for p in Path(path).glob('*.wav'))\n",
    "\n",
    "    for i, file_path in enumerate(walker):\n",
    "        path,filename = os.path.split(file_path)\n",
    "        speaker, _ = os.path.splitext(filename)\n",
    "        speaker_id, utterance_number = speaker.split('_nohash_')\n",
    "        utternance_number = int(utterance_number)\n",
    "\n",
    "        # Load audio\n",
    "        waveform, sample_rate = torchaudio.load(file_path)\n",
    "        dataset.append([waveform, sample_rate, label, speaker_id, utterance_number])\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the `load_audio_files` function to load the contents from each of the audio class files, as well as their metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset_speechcommands_yes = load_audio_file('./data/SpeechCommands/speech_commands_v0.02/yes', 'yes')\n",
    "trainset_speechcommands_no = load_audio_file('./data/SpeechCommands/speech_commands_v0.02/no', 'no')\n",
    "\n",
    "print(f'Length of yes dataset: {len(trainset_speechcommands_yes)}')\n",
    "print(f'Length of no dataset: {len(trainset_speechcommands_no)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now load the dataset into a data loader for both `yes` and `no` training sample sets. `DataLoader` sets the number os batches you want to iterate to load the dataset thorugh your network, to train the model. We'll set the batch size to 1, because we want to load the entire batch in one teration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traubkiader_yes = torch.utils.data.DataLoader(trainset_speechcommands_yes, batch_size=1, shuffle=True, num_workers=0)\n",
    "traubkiader_no = torch.utils.data.DataLoader(trainset_speechcommands_no, batch_size=1, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see how the data looks, we'll grab the waveform and sample rate form each class, and print out a sample of the dataset.\n",
    "* THe **waveform** value is n a Tensor with a float datatype.\n",
    "* The **sample_rate** value is 16000 in the format the audio signal was captured.\n",
    "* The **label** value is the command classification of the word uttered in the audio, `yes` or `no`.\n",
    "* The **ID** is a unique identifier of the audio file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yes_waveform =  trainset_speechcommands_yes[0][0]\n",
    "yes_sample_rate = trainset_speechcommands_yes[0][1]\n",
    "print(f'Yes waveform: {yes_waveform}')\n",
    "print(f'Yes sample rate: {yes_sample_rate}')\n",
    "print(f'Yes Label: {trainset_speechcommands_yes[0][2]}')\n",
    "print(f'Yes ID: {trainset_speechcommands_yes[0][3]}\\n')\n",
    "\n",
    "no_waveform =  trainset_speechcommands_no[0][0]\n",
    "no_sample_rate = trainset_speechcommands_no[0][1]\n",
    "print(f'No waveform: {no_waveform}')\n",
    "print(f'No sample rate: {no_sample_rate}')\n",
    "print(f'No Label: {trainset_speechcommands_no[0][2]}')\n",
    "print(f'No ID: {trainset_speechcommands_no[0][3]}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform and visulize\n",
    "\n",
    "Let's break down some of the audio transforms and the visualization to better understand what they are, and what they tell us about the data.\n",
    "\n",
    "## Waveform\n",
    "\n",
    "The waveform is generated by taking the sample rate and frequency, and representing the signal visually. This signal can be represented as a `waveform`, which is the `signal` representation over time, in a grapahical format. The audio can be recorded in different `channels`.\n",
    "\n",
    "Here's how to use the `resample` transform to reduce the size of the waveform, and then graph the data to visualize the new waveform shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_waveform(waveform, sample_rate, label):\n",
    "    print(\"Waveform: {}\\nSample rate: {}\\nLabel: {}\".format(waveform, sample_rate, label))\n",
    "    new_sample_rate = sample_rate/10\n",
    "\n",
    "    # Resample applies to a single channel, we resample first channel here.\n",
    "    channel = 0\n",
    "    waveform_transformed = torchaudio.transforms.Resample(sample_rate, new_sample_rate)(waveform[channel,:].view(1,-1))\n",
    "\n",
    "    print(\"Shape of transformed wavrform: {}\\nSample rate: {}\".format(waveform_transformed.size(), new_sample_rate))\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(waveform_transformed[0,:].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dispkayed results show how the sample rate is transformed from 16000 to 1600."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_waveform(yes_waveform, yes_sample_rate, 'yes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_waveform(no_waveform, no_sample_rate, 'no')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spectrogram\n",
    "\n",
    "A spectrogram maps the frequency to time of an audio file, and it sllows you to visualize audio data by frequency. It is an image format. This image is what we'll use for our computer vision classification on the audio files. You can view the spectrogram image in grayscale, or in Red Green Blue (RGB) color format.\n",
    "\n",
    "Every spectrogram image helps show the different features the sound signal produces in a color pattern. The convolutional neural network(CNN) treats the color patterns in the image as features for training the model to classify the audio.\n",
    "\n",
    "Let's use the PyTorch `torchaudio.transforms` function to transform the waveform to a spectrogram image format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_spectrogram(waveform_classA, waveform_classB):\n",
    "    yes_spectrogram = torchaudio.transforms.Spectrogram()(waveform_classA)\n",
    "    print(\"\\nShape of yes spectrogram: {}\".format(yes_spectrogram.size()))\n",
    "\n",
    "    no_spectrogram = torchaudio.transforms.Spectrogram()(waveform_classB)\n",
    "    print(\"\\nShape of no spectrogram: {}\".format(no_spectrogram.size()))\n",
    "\n",
    "    plt.figure()\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.title('Features of {}'.format('no'))\n",
    "    plt.imshow(no_spectrogram.log2()[0,:,:].numpy(), cmap='viridis')\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.title('Features of {}'.format('yes'))\n",
    "    plt.imshow(yes_spectrogram.log2()[0,:,:].numpy(), cmap='viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the waveform for the `yes` command to display the spectrogram images dimensions and color pattern in a RGB chart. We'll also compare the feature difference between the `yes` and `no` audio commands.\n",
    "* The **y-axis** is the frequency of the audio.\n",
    "* The **x-axis** is the time of the audio.\n",
    "* the intensity of the images shows the amplitude of the audio. In the following spectrogram images, the high concentrate of the yellow color illustrates the amplitude of the audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_spectrogram(yes_waveform, no_waveform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mel spectrogram\n",
    "\n",
    "Mel Spectrogram is also a frequency to time, but the frequency is converted to the Mel scale. The Mel scale takes the frequency and changes it, based on the perception of the sound of the scale or melody. This transforms the frequency within to  the Mdel scale, and then creates the spectrogram image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_melspectrogram(waveform, sample_rate):\n",
    "    mel_spectrogram = torchaudio.transforms.MelSpectrogram(sample_rate)(waveform)\n",
    "    print(\"Shape of spectrogram: {}\".format(mel_spectrogram.size()))\n",
    "\n",
    "    plt.figure()\n",
    "    plt.imshow(mel_spectrogram.log2()[0,:,:].numpy(), cmap='viridis')\n",
    "\n",
    "show_melspectrogram(yes_waveform, yes_sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mel-frequenc cepstral coefficients (MFCC)\n",
    "\n",
    "A simplified explanation of what the MFCC does is that it takes our frequency, applies transforms, and the result is the amplitudes of the spectrum created from the frequency. Let's take a look at what this looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_mfcc(waveform, sample_rate):\n",
    "    mfcc_spectrogram = torchaudio.transforms.MFCC(sample_rate)(waveform)\n",
    "    print(\"Shape of spectrogram: {}\".format(mfcc_spectrogram.size()))\n",
    "\n",
    "    plt.figure()\n",
    "    fig1 = plt.gcf() # Get current figure\n",
    "    plt.imshow(mfcc_spectrogram.log2()[0,:,:].numpy(), cmap='viridis')\n",
    "\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(mfcc_spectrogram.log2()[0,:,:].numpy())\n",
    "    plt.draw()\n",
    "\n",
    "show_mfcc(no_waveform, no_sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an image from s spectrogram\n",
    "\n",
    "At this point, you have a better understainding of your audio data, and different transformations you can use on it. Now, let's create the images we will use for classification.\n",
    "\n",
    "The following are two different function to create the spectrogram image or the MFCC images for classification. You will use the spectrogram images to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spectrogram_images(trainloader, label_dir):\n",
    "    directory = f'./data/spectrograms/{label_dir}/'\n",
    "    if (os.path.isdir(directory)):\n",
    "        print(f'Data exists for, {label_dir}')\n",
    "    else:\n",
    "        os.makedirs(directory, mode=0o777, exist_ok=True)\n",
    "        for i, data in enumerate(trainloader):\n",
    "            waveform =data[0]\n",
    "            sample_rate = data[1][0]\n",
    "            label = data[2]\n",
    "            ID =data[3]\n",
    "\n",
    "            # create transformed waveforms\n",
    "            spectrogram_tensor = torchaudio.transforms.Spectrogram()(waveform)\n",
    "            fig=plt.figure()\n",
    "            plt.imsave(f'./data/spectrograms/{label_dir}/spec_img{i}.png', spectrogram_tensor[0].log2()[0,:,:].numpy(), cmap='viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the difine function to create the `MFCC` images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mfcc_images(trainloader, label_dir):\n",
    "    os.makedirs(f'./data/mfcc_spectrograms/{label_dir}/', mode=0o777, exist_ok=True)\n",
    "\n",
    "    for i, data in enumerate(trainloader):\n",
    "        waveform =data[0]\n",
    "        sample_rate = data[1][0]\n",
    "        label = data[2]\n",
    "        ID =data[3]\n",
    "\n",
    "        # create transformed waveforms\n",
    "        mfcc_spectrogram = torchaudio.transforms.MFCC(sample_rate)(waveform)\n",
    "        plt.figure()\n",
    "        fig1=plt.gcf()\n",
    "        plt.imshow(mfcc_spectrogram[0].log2()[0,:,:].numpy(), cmap='viridis')\n",
    "        plt.draw()\n",
    "        fig1.savefig(f'./data/mfcc_spectrograms/{label_dir}/spec_img{i}.png', dpi=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the spectrogram images that you'll use for the audio classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_spectrogram_images(traubkiader_yes, 'yes')\n",
    "create_spectrogram_images(traubkiader_no, 'no')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
