{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data visualization and transformation is an important part of every model. Now that we have our dataset downloaded, let's learn more about audio data visualization and transforming this dataset.\n",
    "\n",
    "TorchAudio has many transformation functions for audio manipulation and feature extractions. However, we will focus on the following concepts and tranforms:\n",
    "* **Spectrogram** Create a spectrogram from a waveform.\n",
    "* **MelSpectrogram** Create Mel Spectrogram from a waveform using the `STFT` function in PyTorch.\n",
    "* **Waveform**\n",
    "* **MFCC** Create the Mel-frequency cepstrum coefficients from a waveform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import torch\n",
    "import torchaudio\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the paltform, Apple Silicon or Linux\n",
    "import os, platform\n",
    "\n",
    "torch_device=\"cpu\"\n",
    "\n",
    "if 'kaggle' in os.environ.get('KAGGLE_URL_BASE','localhost'):\n",
    "    torch_device = 'cuda'\n",
    "else:\n",
    "    torch_device = 'mps' if platform.system() == 'Darwin' else 'cpu'\n",
    "\n",
    "torch_device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll go through the audio file tthat we downloaded in local directory by filtering out the ones that are the `yes` abd `no` commands under the `nohash` path. Then we'll load the files into the `torchaudio` data object. This will make it easy to extract attributes of the audio (for example, the waveform and sample rate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio_file(path:str, label:str):\n",
    "    dataset =[]\n",
    "    walker = sorted(str(p) for p in Path(path).glob('*.wav'))\n",
    "\n",
    "    for i, file_path in enumerate(walker):\n",
    "        path,filename = os.path.split(file_path)\n",
    "        speaker, _ = os.path.splitext(filename)\n",
    "        speaker_id, utterance_number = speaker.split('_nohash_')\n",
    "        utternance_number = int(utterance_number)\n",
    "\n",
    "        # Load audio\n",
    "        waveform, sample_rate = torchaudio.load(file_path)\n",
    "        dataset.append([waveform, sample_rate, label, speaker_id, utterance_number])\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the `load_audio_files` function to load the contents from each of the audio class files, as well as their metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset_speechcommands_yes = load_audio_file('./data/SpeechCommands/speech_commands_v0.02/yes', 'yes')\n",
    "trainset_speechcommands_no = load_audio_file('./data/SpeechCommands/speech_commands_v0.02/no', 'no')\n",
    "\n",
    "print(f'Length of yes dataset: {len(trainset_speechcommands_yes)}')\n",
    "print(f'Length of no dataset: {len(trainset_speechcommands_no)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now load the dataset into a data loader for both `yes` and `no` training sample sets. `DataLoader` sets the number os batches you want to iterate to load the dataset thorugh your network, to train the model. We'll set the batch size to 1, because we want to load the entire batch in one teration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traubkiader_yes = torch.utils.data.DataLoader(trainset_speechcommands_yes, batch_size=1, shuffle=True, num_workers=0)\n",
    "traubkiader_no = torch.utils.data.DataLoader(trainset_speechcommands_no, batch_size=1, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see how the data looks, we'll grab the waveform and sample rate form each class, and print out a sample of the dataset.\n",
    "* THe **waveform** value is n a Tensor with a float datatype.\n",
    "* The **sample_rate** value is 16000 in the format the audio signal was captured.\n",
    "* The **label** value is the command classification of the word uttered in the audio, `yes` or `no`.\n",
    "* The **ID** is a unique identifier of the audio file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yes_waveform =  trainset_speechcommands_yes[0][0]\n",
    "yes_sample_rate = trainset_speechcommands_yes[0][1]\n",
    "print(f'Yes waveform: {yes_waveform}')\n",
    "print(f'Yes sample rate: {yes_sample_rate}')\n",
    "print(f'Yes Label: {trainset_speechcommands_yes[0][2]}')\n",
    "print(f'Yes ID: {trainset_speechcommands_yes[0][3]}\\n')\n",
    "\n",
    "no_waveform =  trainset_speechcommands_no[0][0]\n",
    "no_sample_rate = trainset_speechcommands_no[0][1]\n",
    "print(f'No waveform: {no_waveform}')\n",
    "print(f'No sample rate: {no_sample_rate}')\n",
    "print(f'No Label: {trainset_speechcommands_no[0][2]}')\n",
    "print(f'No ID: {trainset_speechcommands_no[0][3]}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform and visulize\n",
    "\n",
    "Let's break down some of the audio transforms and the visualization to better understand what they are, and what they tell us about the data.\n",
    "\n",
    "## Waveform\n",
    "\n",
    "The waveform is generated by taking the sample rate and frequency, and representing the signal visually. This signal can be represented as a `waveform`, which is the `signal` representation over time, in a grapahical format. The audio can be recorded in different `channels`.\n",
    "\n",
    "Here's how to use the `resample` transform to reduce the size of the waveform, and then graph the data to visualize the new waveform shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_waveform(waveform, sample_rate, label):\n",
    "    print(\"Waveform: {}\\nSample rate: {}\\nLabel: {}\".format(waveform, sample_rate, label))\n",
    "    new_sample_rate = sample_rate/10\n",
    "\n",
    "    # Resample applies to a single channel, we resample first channel here.\n",
    "    channel = 0\n",
    "    waveform_transformed = torchaudio.transforms.Resample(sample_rate, new_sample_rate)(waveform[channel,:].view(1,-1))\n",
    "\n",
    "    print(\"Shape of transformed wavrform: {}\\nSample rate: {}\".format(waveform_transformed.size(), new_sample_rate))\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(waveform_transformed[0,:].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dispkayed results show how the sample rate is transformed from 16000 to 1600."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_waveform(yes_waveform, yes_sample_rate, 'yes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_waveform(no_waveform, no_sample_rate, 'no')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spectrogram\n",
    "\n",
    "A spectrogram maps the frequency to time of an audio file, and it sllows you to visualize audio data by frequency. It is an image format. This image is what we'll use for our computer vision classification on the audio files. You can view the spectrogram image in grayscale, or in Red Green Blue (RGB) color format.\n",
    "\n",
    "Every spectrogram image helps show the different features the sound signal produces in a color pattern. The convolutional neural network(CNN) treats the color patterns in the image as features for training the model to classify the audio.\n",
    "\n",
    "Let's use the PyTorch `torchaudio.transforms` function to transform the waveform to a spectrogram image format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_spectrogram(waveform_classA, waveform_classB):\n",
    "    yes_spectrogram = torchaudio.transforms.Spectrogram()(waveform_classA)\n",
    "    print(\"\\nShape of yes spectrogram: {}\".format(yes_spectrogram.size()))\n",
    "\n",
    "    no_spectrogram = torchaudio.transforms.Spectrogram()(waveform_classB)\n",
    "    print(\"\\nShape of no spectrogram: {}\".format(no_spectrogram.size()))\n",
    "\n",
    "    plt.figure()\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.title('Features of {}'.format('no'))\n",
    "    plt.imshow(no_spectrogram.log2()[0,:,:].numpy(), cmap='viridis')\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.title('Features of {}'.format('yes'))\n",
    "    plt.imshow(yes_spectrogram.log2()[0,:,:].numpy(), cmap='viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the waveform for the `yes` command to display the spectrogram images dimensions and color pattern in a RGB chart. We'll also compare the feature difference between the `yes` and `no` audio commands.\n",
    "* The **y-axis** is the frequency of the audio.\n",
    "* The **x-axis** is the time of the audio.\n",
    "* the intensity of the images shows the amplitude of the audio. In the following spectrogram images, the high concentrate of the yellow color illustrates the amplitude of the audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_spectrogram(yes_waveform, no_waveform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mel spectrogram\n",
    "\n",
    "Mel Spectrogram is also a frequency to time, but the frequency is converted to the Mel scale. The Mel scale takes the frequency and changes it, based on the perception of the sound of the scale or melody. This transforms the frequency within to  the Mdel scale, and then creates the spectrogram image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_melspectrogram(waveform, sample_rate):\n",
    "    mel_spectrogram = torchaudio.transforms.MelSpectrogram(sample_rate)(waveform)\n",
    "    print(\"Shape of spectrogram: {}\".format(mel_spectrogram.size()))\n",
    "\n",
    "    plt.figure()\n",
    "    plt.imshow(mel_spectrogram.log2()[0,:,:].numpy(), cmap='viridis')\n",
    "\n",
    "show_melspectrogram(yes_waveform, yes_sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mel-frequenc cepstral coefficients (MFCC)\n",
    "\n",
    "A simplified explanation of what the MFCC does is that it takes our frequency, applies transforms, and the result is the amplitudes of the spectrum created from the frequency. Let's take a look at what this looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_mfcc(waveform, sample_rate):\n",
    "    mfcc_spectrogram = torchaudio.transforms.MFCC(sample_rate)(waveform)\n",
    "    print(\"Shape of spectrogram: {}\".format(mfcc_spectrogram.size()))\n",
    "\n",
    "    plt.figure()\n",
    "    fig1 = plt.gcf() # Get current figure\n",
    "    plt.imshow(mfcc_spectrogram.log2()[0,:,:].numpy(), cmap='viridis')\n",
    "\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(mfcc_spectrogram.log2()[0,:,:].numpy())\n",
    "    plt.draw()\n",
    "\n",
    "show_mfcc(no_waveform, no_sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an image from s spectrogram\n",
    "\n",
    "At this point, you have a better understainding of your audio data, and different transformations you can use on it. Now, let's create the images we will use for classification.\n",
    "\n",
    "The following are two different function to create the spectrogram image or the MFCC images for classification. You will use the spectrogram images to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spectrogram_images(trainloader, label_dir):\n",
    "    directory = f'./data/spectrograms/{label_dir}/'\n",
    "    if (os.path.isdir(directory)):\n",
    "        print(f'Data exists for, {label_dir}')\n",
    "    else:\n",
    "        os.makedirs(directory, mode=0o777, exist_ok=True)\n",
    "        for i, data in enumerate(trainloader):\n",
    "            waveform =data[0]\n",
    "            sample_rate = data[1][0]\n",
    "            label = data[2]\n",
    "            ID =data[3]\n",
    "\n",
    "            # create transformed waveforms\n",
    "            spectrogram_tensor = torchaudio.transforms.Spectrogram()(waveform)\n",
    "            fig=plt.figure()\n",
    "            plt.imsave(f'./data/spectrograms/{label_dir}/spec_img{i}.png', spectrogram_tensor[0].log2()[0,:,:].numpy(), cmap='viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the difine function to create the `MFCC` images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mfcc_images(trainloader, label_dir):\n",
    "    os.makedirs(f'./data/mfcc_spectrograms/{label_dir}/', mode=0o777, exist_ok=True)\n",
    "\n",
    "    for i, data in enumerate(trainloader):\n",
    "        waveform =data[0]\n",
    "        sample_rate = data[1][0]\n",
    "        label = data[2]\n",
    "        ID =data[3]\n",
    "\n",
    "        # create transformed waveforms\n",
    "        mfcc_spectrogram = torchaudio.transforms.MFCC(sample_rate)(waveform)\n",
    "        plt.figure()\n",
    "        fig1=plt.gcf()\n",
    "        plt.imshow(mfcc_spectrogram[0].log2()[0,:,:].numpy(), cmap='viridis')\n",
    "        plt.draw()\n",
    "        fig1.savefig(f'./data/mfcc_spectrograms/{label_dir}/spec_img{i}.png', dpi=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the spectrogram images that you'll use for the audio classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_spectrogram_images(traubkiader_yes, 'yes')\n",
    "create_spectrogram_images(traubkiader_no, 'no')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the speech model\n",
    "\n",
    "We will be using the `torchvision` package to build the model. The convolutional neural network(CNN) layer (`conv2d`) will be used to extract the unique features from the spectrogram image for each speech command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision import datasets, models, transforms\n",
    "from torchinfo import summary\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load spectrogram images into a data loader for training\n",
    "\n",
    "Here, we provide the path to our image data and use PyTorch's `ImageFolder` dataset helper class to load the images into tensors. We'll also normalize the images by resizing to dimension of 201x81."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './data/spectrograms' #looking in subfolders train\n",
    "yes_no_dataset = datasets.ImageFolder(\n",
    "    root=data_path,\n",
    "    transform=transforms.Compose([transforms.Resize((201,81)),transforms.ToTensor()])\n",
    ")\n",
    "print(yes_no_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Imagefolder` automatically creates the image class labels and indices based on the folders for each audio class. We'll use the `class_to_idx` to view the class mapping for the image dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_map = yes_no_dataset.class_to_idx\n",
    "\n",
    "print('\\nClass category and index of the images: {}\\n'.format(class_map))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data for training and testing\n",
    "\n",
    "We will need to split the data to use 80% to train the model, and 20% to test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data to test and train by using 80% to train\n",
    "train_size = int(0.8 * len(yes_no_dataset))\n",
    "test_size = len(yes_no_dataset) - train_size\n",
    "yes_no_train_dataset, yes_no_test_dataset = torch.utils.data.random_split(yes_no_dataset, [train_size, test_size])\n",
    "\n",
    "print('Ttraining size:', len(yes_no_train_dataset))\n",
    "print('Test size:', len(yes_no_test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the dataset was randomly split, let's count the training data to verify that the data has a fairly even distribution between the images in the `yes` and `no` categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# labels in training set\n",
    "train_classes = [label for _, label in yes_no_train_dataset]\n",
    "Counter(train_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data into the `DataLoader` and specify the batch size of how the data will be divided and loaded in the training iterations. We'll also set the number of workers so specify the number of subprocesses to load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(yes_no_train_dataset, batch_size=15, num_workers=2, shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(yes_no_test_dataset, batch_size=15, num_workers=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at what our training tensor looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "td = train_dataloader.dataset[0][0][0][0]\n",
    "print(td)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the convolutional neural network (CNN) model\n",
    "\n",
    "We will define our layers and parameters:\n",
    "\n",
    "* **conv2d**: Takes an input of 3 `channels`, which represents RGB colors because our input images are in color. The 32 represents the number of feature map images profuced from the convolutional layer. The images are produced after you apply a filter on each image in a channel, with a 5x5 kernel size and a stride of 1. `Max pooling` is set with a 2x2 kernel size to reduce the dimensions of the filtered images. We apply the `ReLU` activation to replace the negative pixel values in the feature map with zero.\n",
    "* **conv2d**: Takes the 32 output images from the previous convolutional layer as input. Then, we increase the output number to 64 feature map images, after a filter is applied on the 32 input images, with a 5x5 kernel size and a stride of 1. `Max pooling` is set with a 2x2 kernel size to reduce the dimensions of the filtered images. We apply the `ReLU` activation to replace the negative pixel values to 0.\n",
    "* **dropout**: Removes some of the features extraced from the `conv2d` layer with the ratio of 0.50 to prevent overfitting.\n",
    "* **flatten**: Converts features from the `con2d` output image into the linear input layer.\n",
    "* **Linear**: Takes a number of 51136 features as input, and sets the number of outpus from the network to be 50 logits. The next layer will take the 50 inputs and produces 2 logits in the outpur layer. The `ReLU` activation function will be applied to the neurons across the linear network to replace the negative values to 0. The 2 output values wil be used to predict the classification `yes` ot `no`.\n",
    "* **log_Softmax**: An activation function applied to the 2 output values to predict the probability of the audio classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNet(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1= nn.Linear(51136, 50)\n",
    "        self.fc2 = nn.Linear(50, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)),2))\n",
    "        #x = x.view(x.size(0), -1)\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "model = CNNet().to(torch_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create train and test functions\n",
    "\n",
    "Now we set the cost function, learning rate and optimizer. Then we define the train and test functions that we will use to train and test the model by using the CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cost function used to determine best parameters\n",
    "cost = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# used to create optimal parameters\n",
    "learning_rate = 0.0001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# create the training function\n",
    "\n",
    "def train(dataloader, model, loss, optimizer):\n",
    "    model.train()\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X,Y) in enumerate(dataloader):\n",
    "        X, Y = X.to(torch_device), Y.to(torch_device)\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(X)\n",
    "        loss = cost(pred, Y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch %100 ==0:\n",
    "            loss, current = loss.item(), batch*len(X)\n",
    "            print(f'loss: {loss:>7f} [{current:>5d}/{size:>5d}]')\n",
    "\n",
    "# Create the validation/test function\n",
    "def test(dataloader, model):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0,0\n",
    "    with torch.no_grad():\n",
    "        for batch, (X,Y) in enumerate(dataloader):\n",
    "            X, Y = X.to(torch_device), Y.to(torch_device)\n",
    "            pred = model(X)\n",
    "            test_loss += cost(pred, Y).item()\n",
    "            correct += (pred.argmax(1)==Y).type(torch.float).sum().item()\n",
    "    test_loss /= size\n",
    "    correct /= size\n",
    "    print(f'Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n",
    "Now let's set the number of epochs, and call our `train` and `test` functions for each iteration. We'll iterate throught the training network by the number of epochs. As we train the model, we'll calcualte the loss as it decreases during the training. In addition, we'll display the accuracy as the optimization increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 15\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f'Epoch {t+1}\\n-------------------------------')\n",
    "    train(train_dataloader, model, cost, optimizer)\n",
    "    test(test_dataloader, model)\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the summary breakdown of the model architecture. It shows the number of filters used for the feature extraction and image reduction from pooling for each convolutional layer. Next, it shows 51136 input features and the 2 outputs used for classification in the linear layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(model, input_size=(15, 3, 201, 81))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model\n",
    "\n",
    "We can get somewhere between a 93-95 percent accuracy by the 15 epoch. Here we grab a batch from our test data, and see how the model performs on the predicated result and the actual result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "test_loss, correct = 0,0\n",
    "class_map = ['no', 'yes']\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch, (X,Y) in enumerate(test_dataloader):\n",
    "        X,Y = X.to(torch_device), Y.to(torch_device)\n",
    "        pred = model(X)\n",
    "        print('Predicted:\\nvalue={}, class_name={}\\n'.format(pred[0].argmax(0), class_map[pred[0].argmax(0)]))\n",
    "        print('Actual:\\nvalue={}, class_name={}\\n'.format(Y[0], class_map[Y[0]]))\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
