{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint variants\n",
    "\n",
    "A checkpoint variant is usually **a checkpoint where it's weights** are:\n",
    "\n",
    "* Stored in a different floating point type for `lower precision` and `lower storage`, such as `torch.float16`, because it only requires half the bandwidth and storage to download. You cann't use this variant if you're continuing training or using a CPU.\n",
    "\n",
    "* `Non-exponential mean averaged(EMA)` weights which shouldn't be used for inference. You should use these to continue `finetuning a model`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otherwise, a variant is `identical` to the original checkpoint. They have exactly the same seialization format, model structure, and weights have identical tensor shapes.\n",
    "\n",
    "checkpoint type | weight name | argument for loading weights\n",
    "--- | --- | ---\n",
    "original | diffusion_pytorch_model.bin |\n",
    "floating point | diffusion_pytorch_fp16_model.bin | variant, torch_dtype\n",
    "non-EMA| diffusion_pytorch_ema_model.non_ema.bin | variant\n",
    "\n",
    "\n",
    "There are two important arguments to know for loading variants:\n",
    "* `torch_dtype` defines the floating point precision od the loaded checkpoints. For example, if you want to save bandwidth by loading a fp16 variant, you should specify torch_dtype=torch.float16 to `convert the weights` to fp16. Otherwise, the fp16 weights are convertd to the default fp32 precision. You can also load the original checkpoint without defining the variant argument, and convert it to fp16 with torch_dtype=torch.float16. In this case, the default fp32 weights are downloaded first, and then they'are converted to fp16 after loading.\n",
    "\n",
    "* `variant` defines which files should be loaded from the repository. For example, if you want to load a non_ema variant from the diffusers/stable-diffusion-variants repository, you should specify=\"non-ema\" to download the non_ema files."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DiffusionPipeline\n",
    "import torch\n",
    "\n",
    "repo_id=\"runwayml/stable-diffusion-v1-5\"\n",
    "\n",
    "# load fp16 variant\n",
    "stable_diffusion = DiffusionPipeline.from_pretrained(repo_id, variant='fp16', torch_dtype=torch.float16)\n",
    "\n",
    "# load non_ema variant\n",
    "# stable_diffusion = DiffusionPipeline.from_pretrained(repo_id, variant='non_ema')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving a variant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as fp16 variant\n",
    "stable_diffusion.save_pretrained(repo_id, variant='fp16')\n",
    "\n",
    "# save as non-ema variant\n",
    "# stable_diffusion.save_pretrained(repo_id, variant='non_ema')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the fp16 variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stable_diffusion = DiffusionPipeline.from_pretrained(repo_id, variant='fp16', torch_dtype=torch.float16)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models are loaded from the `ModelMixin.from_pretrained()` method, **which downloads and caches the latest version of the model weights and configurations.** If the latest files are available in the local cache, from_pretrained() reuses files in the cache instead of redownloading them.\n",
    "\n",
    "Models can be loaded from a subfolder with the subfolder argument. For example, the model weights for runwayml/stable-diffusion-v1-5 are stored in the unet subfolder:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the `unet` model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import UNet2DConditionModel\n",
    "\n",
    "model=UNet2DConditionModel.from_pretrained(repo_id, subfolder='unet')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading from a repository's directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import UNet2DModel\n",
    "\n",
    "repo2_id=\"google/ddpm-cifar10-32\"\n",
    "\n",
    "model=UNet2DModel.from_pretrained(repo2_id,)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schedulers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Schedulers are loaded from the `SchedulerMixin.from_pretrained()` method, and unlike models, schedulers are not `parameterized` or `trained`; they are defined by a configuration file.\n",
    "\n",
    "**Loading schedulers does not consume any significant amount of memory and the same configuration file can be used for a variety of different schedulers.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionPipeline\n",
    "from diffusers import (\n",
    "    DDPMScheduler,\n",
    "    DDIMScheduler,\n",
    "    PNDMScheduler,\n",
    "    LMSDiscreteScheduler,\n",
    "    EulerDiscreteScheduler,\n",
    "    EulerAncestralDiscreteScheduler,\n",
    "    DPMSolverMultistepScheduler,\n",
    ")\n",
    "\n",
    "ddpm = DDPMScheduler.from_pretrained(repo_id, subfolder='scheduler')\n",
    "ddim = DDIMScheduler.from_pretrained(repo_id, subfolder='scheduler')\n",
    "pndm = PNDMScheduler.from_pretrained(repo_id, subfolder='scheduler')\n",
    "lms = LMSDiscreteScheduler.from_pretrained(repo_id, subfolder='scheduler')\n",
    "euler = EulerDiscreteScheduler.from_pretrained(repo_id, subfolder='scheduler')\n",
    "euler_ancestral = EulerAncestralDiscreteScheduler.from_pretrained(repo_id, subfolder='scheduler')\n",
    "dpm_multistep = DPMSolverMultistepScheduler.from_pretrained(repo_id, subfolder='scheduler')\n",
    "\n",
    "pipeline = StableDiffusionPipeline.from_pretrained(repo_id, scheduler=\"<any of the scheduler's name below'>\", torch_dtype=torch.float16)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DiiffusionPipeline explained\n",
    "\n",
    "As a class method, DiffusionPipeline.from_pretrained() is responsible for two things:\n",
    "* Download the latest version of the folder strcuture required for inference and cache it. If the latest folder structure is avaliable inthe local cache, DiffusionPipeline.from_pretrained() reuses files in the cache instead of redownloading them.\n",
    "* Load the cached weights into the correct pipline class, retrieved from the model_index.json file and return an instance of it.\n",
    "\n",
    "\n",
    "The pipelines underlying folder structure corresponds directly with their class instances. i.e. the StableDiffusionPipeline corresponds to the folder structure in runwayml/stable-diffusion-v1-5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DiffusionPipeline\n",
    "\n",
    "pipeline = DiffusionPipeline.from_pretrained(repo_id, torch_dtype=torch.float16)\n",
    "print(pipeline)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will see pipeline is an insrance of StableDiffusionPipline, which consists of seven components:\n",
    "\n",
    "* \"feature_extractor\": a CLIPFeatureExtractor from transformers\n",
    "* \"safety_checker\": a component from screening against harmful content\n",
    "* \"scheduler\": an instance of PNDMScheduler\n",
    "* \"text_encoder\": a CLIPTextEncoder from transformers\n",
    "* \"tokenizer\": a CLIPTokenizer from transformers\n",
    "* \"unet\": an instance of UNet2DConditionModel\n",
    "* \"vae\": an instance of AutoencoderKL"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access each of the components of the pipeline as an attribute to view its configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.tokenizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every pipeline expects a model_index.json file that tells the DiffusionPipeline:\n",
    "* which pipeline class to load from _class_ name\n",
    "* which version of Diffusers was used to create the model in _diffusers_ version\n",
    "* what compoents from which library are stored in the subfolders(name corresponds to the component and subfolder name, library corresponds to the name of the library to load the class from, and class corresponds to the class name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
