{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diffusion pipeline are inherently a collection of diffusion models and schedulers that are partly independent from each other. This means that one is able to switch out parts of the pipeline to better customize a pipeline to one's use case. The best example of this is the Schedulers.\n",
    "\n",
    "Whereas diffusion models usually simply define the forward pass from noise to a less noisy sample, schedulers define the whole denoising process, i.e.:\n",
    "* How many denosing steps?\n",
    "* Stochastic or deterministic?\n",
    "* What algorithm to use to find the denoised sample\n",
    "\n",
    "They can be quite complex and often define a trade-off between **denoising speed** and **denoising quality**. It is extremly difficult to measure quantiatively which shceduler works best for a given diffusion pipeline, so it is often recommended to simplu try out which works best."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hard to lock to the specific version of dependencies https://github.com/huggingface/diffusers/issues/1255\n",
    "!pip install --upgrade git+https://github.com/huggingface/diffusers.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, platform\n",
    "\n",
    "torch_device = 'cpu'\n",
    "\n",
    "if 'kaggle' in os.environ.get('KAGGLE_URL_BASE', 'localhost'):\n",
    "    torch_device = 'cuda'\n",
    "else:\n",
    "    torch_device = 'mps' if platform.system() == 'Darwin' else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DiffusionPipeline\n",
    "import torch\n",
    "\n",
    "\n",
    "repo_id = \"runwayml/stable-diffusion-v1-5\"\n",
    "\n",
    "#fp16 did not support by CPU inference sometimes\n",
    "pipeline = DiffusionPipeline.from_pretrained(repo_id, torch_dtype=torch.float16, device=torch_device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Move to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.to(torch_device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access the scheduler"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scheduler is always one of the components of the pipeline and is usually called \"scheduler\". So it can be accessed via the \"scheduler\" property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.scheduler"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the scheduler is of type PNDMScheduler. Cool, now let's compare the scheduler in its performance to other schedulers. First we define a prompt on which we will test all the different schedulers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'A photograph of an astronaut riding a horse on Mars, high resolution, high definition.'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create a generator from a random seed that will ensure that we can generate smimilar images as well as run the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = torch.Generator(device=torch_device).manual_seed(8)\n",
    "image = pipeline(prompt, generator=generator).images[0]\n",
    "image"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing the scheduler\n",
    "\n",
    "Now we show how easy it is to change the scheduler or a pipeline. Every scheduler has a property SchedulerMixin. compatibles which defines all compatible schedulers. You can take a look at all available, compatible schedulers for the Stable Diffusion pipeline as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.scheduler.compatibles"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing the input prompt with all other schedulers\n",
    "\n",
    "To change the scheduler of the pipeline you can make use of the convenient ConfigMixin.config property in combination with the ConfigMixin.from_config() function, return a dictionary of the configuration of the scheduler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.scheduler.config"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This configuration can then be used to instantiate a shcuduler of a different class that is compatible with the pipeline. Here, we change the scheduler to the DDIMScheduler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DDIMScheduler\n",
    "\n",
    "pipeline.scheduler = DDIMScheduler.from_config(pipeline.scheduler.config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, now we can run the pipeline again to compare the genertion quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = torch.Generator(device=torch_device).manual_seed(8)\n",
    "image = pipeline(prompt, generator=generator).images[0]\n",
    "image"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare schedulers\n",
    "\n",
    "So far we have tried running the stable diffusion pipeline with two schedulers:\n",
    "* PNDMScheduler\n",
    "* DDIMScheduler"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LMSDiscreteScheduler usually leads to **better results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import LMSDiscreteScheduler\n",
    "\n",
    "pipeline.scheduler = LMSDiscreteScheduler.from_config(pipeline.scheduler.config)\n",
    "\n",
    "generator = torch.Generator(device=torch_device).manual_seed(8)\n",
    "image= pipeline(prompt, generator=generator).images[0]\n",
    "image"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EulerDiscreteScheduler and EulerAncestralDiscreteScheduler can generate **high quality** results with as little as 30 steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import EulerDiscreteScheduler\n",
    "\n",
    "pipeline.scheduler = EulerDiscreteScheduler.from_config(pipeline.scheduler.config)\n",
    "\n",
    "generator = torch.Generator(device=torch_device).manual_seed(8)\n",
    "image = pipeline(prompt, generator=generator, num_inference_steps=30).images[0]\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import EulerAncestralDiscreteScheduler\n",
    "\n",
    "pipeline.scheduler = EulerAncestralDiscreteScheduler.from_config(pipeline.scheduler.config)\n",
    "\n",
    "generator = torch.Generator(device=torch_device).manual_seed(8)\n",
    "image = pipeline(prompt, generator=generator, num_inference_steps=30).images[0]\n",
    "image"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DPMSolverMultistepScheduler give arguably the **best speed/quality trade-off** and can be run with as little as 20 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DPMSolverMultistepScheduler\n",
    "\n",
    "pipeline.scheduler = DPMSolverMultistepScheduler.from_config(pipeline.scheduler.config)\n",
    "\n",
    "generator = torch.Generator(device=torch_device).manual_seed(8)\n",
    "image = pipeline(prompt, generator=generator, num_inference_steps=30).images[0]\n",
    "image"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "As you can see most images look very similar and are arguably of very similar quality. It often reallt dependes on the specific use case which scheduler to choose. A good approach is always to run multiple shcedulers to compare results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
