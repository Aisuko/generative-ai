{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "`Unconditional image generation` is a popular application that generates images that look like those in the dataset used for training. Typically, the best result are obtained from finetuning a pretrained model on s specific dataset. You can find many of these checkpoints on the Huggingface model hub. Let's take a look at how to finetune a pretrained model on a custom dataset. Here we use UNet2DModel from scrach on a subnet of the Smithsonian butterflies dataset to generate images of butterflies."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the environment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start, make sure we have Datasets installed to load and preprocess image datasets, and accelerate.\n",
    "The following command will install TensorBoard to visualize training metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure accelerate supports multiples\n",
    "!pip install accelerate==0.20.3\n",
    "!pip install diffusers['training']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Login to your account for share model(optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The training process"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training configuration\n",
    "\n",
    "For convenience, create a TrainingConfig class containing the training **[hyperparameters](https://aisuko.gitbook.io/wiki/ai-techniques/large-language-model/ggml#hyperparameters)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    image_size = 128 # the generated image resolution\n",
    "    train_batch_size=16\n",
    "    eval_batch_size=16 # how many images to sample for evaluation\n",
    "    num_epochs=50\n",
    "    gradient_accumulation_steps=1\n",
    "    learning_rate=1e-4\n",
    "    lr_warmup_steps=500\n",
    "    save_image_epochs=10\n",
    "    save_model_epochs=30\n",
    "    mixed_precision=\"fp16\" # `no` for float32, `fp16` for automatic mixed precision\n",
    "    output_dir=\"ddpm-butterflies-128\" # the model name locally and on the HF Hub\n",
    "    push_to_hub=False # whether to upload the model to the HF Hub\n",
    "    hub_private_repo=False\n",
    "    overwrite_output_dir=True # overwrite the old model when re-running the notebook\n",
    "    seed=0\n",
    "\n",
    "config = TrainingConfig()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "config.dataset_name = \"huggan/smithsonian_butterflies_subset\"\n",
    "dataset = load_dataset(config.dataset_name, split=\"train\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets uses the image feature to automatically decode the image data and load it as a PIL.Image which we can visualize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs=plt.subplots(1,4,figsize=(16,4))\n",
    "for i, image in enumerate(dataset[:4][\"image\"]):\n",
    "    axs[i].imshow(image)\n",
    "    axs[i].set_axis_off()\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The images are all diferent sizes though, so we'll need to preprocess them first:\n",
    "* `Resize` changes the image size to the one defined in config.image_size\n",
    "* `RandomHorizontalFlip` arguments the dataset by randomly mirroring the images.\n",
    "* [`Normalize`](https://aisuko.gitbook.io/wiki/ai-techniques/framework/ml_training_components#normalization) is important to rescale the pixel values into a [-1,1] range, which is what the model expects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "preprocess = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((config.image_size, config.image_size)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(degrees=10),  # Add random rotation,\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5], [0.5]),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Datasets' set transform methos to apply the `preprocess` function on the fly during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(examples):\n",
    "    images = [preprocess(image.convert(\"RGB\")) for image in examples[\"image\"]]\n",
    "    return {\"images\": images}\n",
    "\n",
    "dataset.set_transform(transform)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Optional)Visualize the images again to confirm that they've been resized. Now you're ready to wrap the dataset in a DataLoader for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=config.train_batch_size,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a UNet2DModel\n",
    "\n",
    "Preatained models in Diffusers are easily created from their model class with the parameters you want. For example, to create a UNet2DModel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import UNet2DModel\n",
    "\n",
    "model=UNet2DModel(\n",
    "    sample_size=config.image_size, # the target image resolution\n",
    "    in_channels=3, # the number of input channels, 3 for RGB images\n",
    "    out_channels=3, # the number of output channels\n",
    "    layers_per_block=2, # how many ResNet layers to use per UNet block\n",
    "    block_out_channels=(128,128,256,256,512,512), # the numbe of output channels for eaxh UNet block\n",
    "    down_block_types=(\n",
    "        \"DownBlock2D\", # a regular ResNet downsampling block\n",
    "        \"DownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "        \"AttnDownBlock2D\", # a ResNet downsampling block with spatial self-attention\n",
    "        \"DownBlock2D\",\n",
    "    ),\n",
    "    up_block_types=(\n",
    "        \"UpBlock2D\", # a regular ResNet upsampling block\n",
    "        \"AttnUpBlock2D\", # a ResNet upsampling block with spatial self-attention\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the sample image shape matches the model output shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_image = dataset[0][\"images\"].unsqueeze(0)\n",
    "print(\"Input shape:\", sample_image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Output shape:\", model(sample_image, timestep=0).sample.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add some noise to the image"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a scheduler"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scheduler behaves differently dependeing on whether you're using the model for training of inference.\n",
    "\n",
    "* `During inference`, the scheduler generates image from the noise. \n",
    "* `During training`, the scheduler takes a model output or a sample from a specific point in the diffusion process and applies noise to the image according to a `noise schedule` and an `update rule`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from PIL import Image\n",
    "from diffusers import DDPMScheduler\n",
    "\n",
    "noise_scheduler = DDPMScheduler(num_train_timesteps=1000)\n",
    "noise=torch.randn(sample_image.shape)\n",
    "timesteps=torch.LongTensor([50])\n",
    "noisy_image=noise_scheduler.add_noise(sample_image, noise, timesteps)\n",
    "\n",
    "Image.fromarray(((noisy_image.permute(0,2,3,1)+1.0)*127.5).type(torch.uint8).numpy()[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training objective of the model is to predict the noise added to the image. The loss at this step can be calculated by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "noise_pred = model(noisy_image, timesteps).sample\n",
    "mse_loss = F.mse_loss(noise_pred, noise)\n",
    "l1_loss = F.l1_loss(noise_pred, noise)\n",
    "combined_loss = 0.7 * mse_loss + 0.3 * l1_loss  # Combine MSE and L1 losses\n",
    " "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, we will need an optimizer and a learning rate scheduler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers.optimization import get_cosine_schedule_with_warmup\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate)\n",
    "lr_scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=config.lr_warmup_steps,\n",
    "    num_training_steps=len(train_dataloader) * config.num_epochs,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second, we will need a way to evaluate the model.\n",
    "\n",
    "For evaluation, we can use the DDPMPipeline to generate a batch of sample images and save it as a grid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DDPMPipeline\n",
    "import math\n",
    "import os\n",
    "\n",
    "def make_grid(images, rows, cols):\n",
    "    w, h =images[0].size\n",
    "    grid = Image.new('RGB', size=(cols * w, rows * h))\n",
    "    for i, image in enumerate(images):\n",
    "        grid.paste(image, box=(i % cols * w, i // cols * h))\n",
    "    return grid\n",
    "\n",
    "def evaluate(config, epoch, pipeline):\n",
    "    # Sample some image from random noise(this is the backward diffusion process).\n",
    "    # The default pipeline output type is `List[PIL.Image]`\n",
    "    images =pipeline(\n",
    "        batch_size=config.eval_batch_size,\n",
    "        generator = torch.manual_seed(config.seed),\n",
    "    ).images\n",
    "\n",
    "    # Make a grid out of the images\n",
    "    image_grid = make_grid(images, rows=4, cols=4)\n",
    "\n",
    "    # Save the images\n",
    "    test_dir=os.path.join(config.output_dir, \"samples\")\n",
    "    os.makedirs(test_dir, exist_ok=True)\n",
    "    image_grid.save(f\"{test_dir}/{epoch:04d}.png\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapping all in a training loop\n",
    "\n",
    "We can use Huggingface `Accelerate` for TensorBoard logging, gradient accumulation,and mixed precision training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from accelerate import Accelerator\n",
    "from huggingface_hub import HfFolder, Repository, whoami\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "\n",
    "def get_full_repo_name(model_id:str, organization:str=None, token:str=None):\n",
    "    if token is None:\n",
    "        token=HfFolder.get_token()\n",
    "    if organization is None:\n",
    "        username = whoami(token=token)[\"name\"]\n",
    "        return f\"{username}/{model_id}\"\n",
    "    else:\n",
    "        return f\"{organization}/{model_id}\"\n",
    "    \n",
    "def train_loop(config, model, noise_scheduler, optimizer, train_dataloader, lr_scheduler):\n",
    "    # Initialize accelerato and tensorboard logging\n",
    "    accelerator  = Accelerator(\n",
    "        mixed_precision=config.mixed_precision,\n",
    "        gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "        log_with=\"tensorboard\",\n",
    "        project_dir=os.path.join(config.output_dir, \"logs\"),\n",
    "    )\n",
    "    if accelerator.is_main_process:\n",
    "        if config.push_to_hub:\n",
    "            repo_name=get_full_repo_name(Path(config.output_dir).name)\n",
    "            repo=Repository(config.output_dir, clone_from=repo_name)\n",
    "        elif config.output_dir is not None:\n",
    "            os.makedirs(config.output_dir, exist_ok=True)\n",
    "        accelerator.init_trackers(\"train_example\")\n",
    "    \n",
    "    # Prepare all the components\n",
    "    # There is no specific order to remember, you just need to unpack the objects in the same order you gave them to the prepare method\n",
    "    model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
    "        model,\n",
    "        optimizer,\n",
    "        train_dataloader,\n",
    "        lr_scheduler,\n",
    "    )\n",
    "\n",
    "    global_step = 0 #the global step across all epochs\n",
    "\n",
    "    # Now we train the model\n",
    "    for epoch in range(config.num_epochs):\n",
    "        progress_bar =tqdm(total=len(train_dataloader), disable=not accelerator.is_local_main_process)\n",
    "        progress_bar.set_description(f\"Epoch {epoch}\")\n",
    "\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            clean_images=batch[\"images\"]\n",
    "            # Sample noise to add the images\n",
    "            noise=torch.randn(clean_images.shape).to(clean_images.device)\n",
    "            bs=clean_images.shape[0]\n",
    "\n",
    "            # Sample a random temstep for each image\n",
    "            timesteps=torch.randint(0, noise_scheduler.config.num_train_timesteps, size=(bs,), device=clean_images.device).long()\n",
    "\n",
    "            # Add noise to the clean images according to the noise magnitude at each timestep (this is the forward diffusion process)\n",
    "            noisy_images=noise_scheduler.add_noise(clean_images, noise, timesteps)\n",
    "\n",
    "            with accelerator.accumulate(model):\n",
    "                # Predict the noise residual\n",
    "                noise_pred = model(noisy_images, timesteps, return_dict=False)[0]\n",
    "                loss=F.mse_loss(noise_pred, noise)\n",
    "                accelerator.backward(loss)\n",
    "\n",
    "                accelerator.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "            progress_bar.update(1)\n",
    "            logs={\"loss\": loss.detach().item(),\"lr\": lr_scheduler.get_last_lr()[0],\"step\": global_step}\n",
    "            progress_bar.set_postfix(**logs)\n",
    "            accelerator.log(logs,step=global_step)\n",
    "            global_step += 1\n",
    "\n",
    "        \n",
    "        # After each epoch you optionally sample some demo images with evaluate() and save the model\n",
    "        if accelerator.is_main_process:\n",
    "            pipeline = DDPMPipeline(unet=accelerator.unwrap_model(model), scheduler=noise_scheduler)\n",
    "\n",
    "            if (epoch+1) % config.save_image_epochs == 0 or epoch == config.num_epochs - 1:\n",
    "                evaluate(config, epoch, pipeline)\n",
    "            \n",
    "            if (epoch +1) % config.save_model_epochs == 0 or epoch == config.num_epochs - 1:\n",
    "                if config.push_to_hub:\n",
    "                    repo.push_to_hub(commit_message=f\"Epoch {epoch}\", blocking=True)\n",
    "                else:\n",
    "                    pipeline.save_pretrained(config.output_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch the training with Accelerate's notebook_launcher function\n",
    "\n",
    "Pass the function to the training loop, all the training arguments and the number of processes(this value to the number of GPUs avaliable to you) to use for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import notebook_launcher\n",
    "\n",
    "args = (config, model, noise_scheduler, optimizer, train_dataloader, lr_scheduler)\n",
    "\n",
    "# TPU num_processes=8, multiples CPUs num_processes=2\n",
    "notebook_launcher(train_loop, args, num_processes=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the final images generated by the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "sample_images=sorted(glob.glob(f\"{config.output_dir}/samples/*.png\"))\n",
    "Image.open(sample_images[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model on Test Data\n",
    "\n",
    "Let's evaluate the trained model on the test dataset and visualize some of the generated images. This will give us an idea of how well the model has learned to generate images.\n",
    "\n",
    "We'll use the same process as before, where we add noise to clean images and then subtract the predicted noise from them to generate the final images.\n",
    "\n",
    "Let's proceed with the evaluation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on test data\n",
    "test_dataset = load_dataset(config.dataset_name, split=\"test\")  # Assuming a \"test\" split exists\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=config.eval_batch_size,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test dataset and generate samples\n",
    "test_samples = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        clean_images = batch[\"images\"]\n",
    "        noise = torch.randn(clean_images.shape).to(clean_images.device)\n",
    "        timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, size=(clean_images.shape[0],), device=clean_images.device).long()\n",
    "\n",
    "        noisy_images = noise_scheduler.add_noise(clean_images, noise, timesteps)\n",
    "        predicted_noise = model(noisy_images, timesteps, return_dict=False)[0]\n",
    "\n",
    "        generated_images = torch.clamp(noisy_images - predicted_noise, -1.0, 1.0)\n",
    "        test_samples.append(generated_images)\n",
    "\n",
    "# Display a few generated images from the test dataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(1, 4, figsize=(16, 4))\n",
    "for i, generated_image in enumerate(test_samples[0][:4]):\n",
    "    axs[i].imshow(generated_image.permute(1, 2, 0).cpu().numpy())\n",
    "    axs[i].set_axis_off()\n",
    "fig.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
